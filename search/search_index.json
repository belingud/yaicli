{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"YAICLI: Your AI Assistant in Command Line","text":"<p>YAICLI is a powerful yet lightweight command-line AI assistant that brings the capabilities of Large Language Models (LLMs) like GPT-4o directly to your terminal. Interact with AI through multiple modes: have natural conversations, generate and execute shell commands, or get quick answers without leaving your workflow.</p> <p>Supports both standard and deep reasoning models across all major LLM providers.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":""},{"location":"#multiple-interaction-modes","title":"\ud83d\udd04 Multiple Interaction Modes","text":"<ul> <li>\ud83d\udcac Chat Mode: Engage in persistent conversations with full context tracking</li> <li>\ud83d\ude80 Execute Mode: Generate and safely run OS-specific shell commands</li> <li>\u26a1 Quick Query: Get instant answers without entering interactive mode</li> </ul>"},{"location":"#smart-environment-awareness","title":"\ud83e\udde0 Smart Environment Awareness","text":"<ul> <li>Auto-detection: Identifies your shell (bash/zsh/PowerShell/CMD) and OS</li> <li>Safe Command Execution: Verification before running any command</li> <li>Flexible Input: Pipe content directly (<code>cat log.txt | ai \"analyze this\"</code>)</li> </ul>"},{"location":"#universal-llm-compatibility","title":"\ud83d\udd0c Universal LLM Compatibility","text":"<ul> <li>OpenAI-Compatible: Works with any OpenAI-compatible API endpoint</li> <li>Multi-Provider Support: Support multiple providers</li> </ul>"},{"location":"#enhanced-terminal-experience","title":"\ud83d\udcbb Enhanced Terminal Experience","text":"<ul> <li>Real-time Streaming: See responses as they're generated with cursor animation</li> <li>Rich History Management: Manage histories with 500 entries by default</li> <li>Syntax Highlighting: Beautiful code formatting with customizable themes</li> </ul>"},{"location":"#developer-friendly","title":"\ud83d\udee0\ufe0f Developer-Friendly","text":"<ul> <li>Layered Configuration: Environment variables &gt; Config file &gt; Sensible defaults</li> <li>Debugging Tools: Verbose mode with detailed API tracing</li> </ul>"},{"location":"#function-calling","title":"\ud83d\udcda Function Calling","text":"<ul> <li>Function Calling: Enable function calling in API requests</li> <li>Function Output: Show the output of functions</li> </ul>"},{"location":"#mcp-calling","title":"\ud83d\udcda MCP Calling","text":"<ul> <li>MCP Calling: Call LLM with MCP tools</li> <li>MCP Output: Show the output of MCP tools</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation</li> <li>Basic Usage</li> <li>Advanced Features</li> <li>Contributing</li> <li>GitHub Repository</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#078-2025-07-03","title":"0.7.8 - 2025-07-03","text":""},{"location":"changelog/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>update CHANGELOG with new release notes - (78761d4) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update fastmcp dependency to 2.10.1 - (9fb6818) - Belingud</li> </ul>"},{"location":"changelog/#077-2025-07-01","title":"0.7.7 - 2025-07-01","text":""},{"location":"changelog/#features","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add fetch_webpage buildin function - (09ec366) - Belingud</li> </ul>"},{"location":"changelog/#documentation_1","title":"\ud83d\udcda Documentation","text":"<ul> <li>update CHANGELOG with new release notes and documentation updates - (2ff0056) - Belingud</li> </ul>"},{"location":"changelog/#076-2025-07-01","title":"0.7.6 - 2025-07-01","text":""},{"location":"changelog/#features_1","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add default role configuration - (91c72f0) - Belingud</li> </ul>"},{"location":"changelog/#refactor","title":"\ud83d\ude9c Refactor","text":"<ul> <li>remove unused imports and trailing whitespace - (cad95fe) - Belingud</li> </ul>"},{"location":"changelog/#documentation_2","title":"\ud83d\udcda Documentation","text":"<ul> <li>add DEFAULT_ROLE to README.md - (6308eea) - Belingud</li> <li>update README.md with improved descriptions and requirements - (25e32bc) - Belingud</li> <li>update README with new provider and formatting - (da056ca) - Belingud</li> <li>update CHANGELOG with v0.7.5 release notes - (566a85c) - Belingud</li> </ul>"},{"location":"changelog/#075-2025-07-01","title":"0.7.5 - 2025-07-01","text":""},{"location":"changelog/#features_2","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add get_weather buildin function - (876efb7) - Belingud</li> </ul>"},{"location":"changelog/#documentation_3","title":"\ud83d\udcda Documentation","text":"<ul> <li>add Spark and Together to README - (e26ea59) - Belingud</li> <li>update CHANGELOG with v0.7.4 release notes - (966e443) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_1","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>remove pydantic deprecation warnings - (ce2c18c) - Belingud</li> <li>add warnings import and filter deprecation warnings - (7003144) - Belingud</li> </ul>"},{"location":"changelog/#074-2025-06-30","title":"0.7.4 - 2025-06-30","text":""},{"location":"changelog/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>handle LLM client creation errors gracefully - (6c8cbed) - Belingud</li> </ul>"},{"location":"changelog/#documentation_4","title":"\ud83d\udcda Documentation","text":"<ul> <li>add MCP Calling documentation to README - (70b5a86) - Belingud</li> <li>update CHANGELOG.md for v0.7.3 release - (ddae983) - Belingud</li> </ul>"},{"location":"changelog/#testing","title":"\ud83e\uddea Testing","text":"<ul> <li>add Together provider tests and improve line formatting - (560330e) - Belingud</li> </ul>"},{"location":"changelog/#073-2025-06-29","title":"0.7.3 - 2025-06-29","text":""},{"location":"changelog/#features_3","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add bailian, bailian-intl, spark and nvida providers - (106214b) - Belingud</li> </ul>"},{"location":"changelog/#documentation_5","title":"\ud83d\udcda Documentation","text":"<ul> <li>add documentation for new providers - (5fd9662) - Belingud</li> <li>update CHANGELOG.md for v0.7.2 release - (44b8702) - Belingud</li> </ul>"},{"location":"changelog/#testing_1","title":"\ud83e\uddea Testing","text":"<ul> <li>add new provider types to TestProviderFactory - (dbf4572) - Belingud</li> <li>add tests for Bailian, Mistral, Nvidia, and Spark providers - (f75a7f6) - Belingud</li> </ul>"},{"location":"changelog/#072-2025-06-28","title":"0.7.2 - 2025-06-28","text":""},{"location":"changelog/#features_4","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add MCP tools error handling and reasoning effort options - (68b9258) - Belingud</li> </ul>"},{"location":"changelog/#bug-fixes_1","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>handle errors in get_openai_mcp_tools and MCPClient methods - (251961f) - Belingud</li> </ul>"},{"location":"changelog/#refactor_1","title":"\ud83d\ude9c Refactor","text":"<ul> <li>standardize completion parameter keys across providers - (f294f75) - Belingud</li> </ul>"},{"location":"changelog/#documentation_6","title":"\ud83d\udcda Documentation","text":"<ul> <li>add Mistral to README - (0d72f37) - Belingud</li> <li>update CHANGELOG.md for v0.7.1 release - (72a2107) - Belingud</li> </ul>"},{"location":"changelog/#testing_2","title":"\ud83e\uddea Testing","text":"<ul> <li>add tests for MistralProvider and update imports - (c297bef) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_2","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>add mistralai and tox-uv dependencies - (d753356) - Belingud</li> </ul>"},{"location":"changelog/#071-2025-06-27","title":"0.7.1 - 2025-06-27","text":""},{"location":"changelog/#documentation_7","title":"\ud83d\udcda Documentation","text":"<ul> <li>update CHANGELOG.md for v0.7.0 release - (782e0de) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_3","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>add fastmcp dependency - (e1dfcbb) - Belingud</li> </ul>"},{"location":"changelog/#070-2025-06-27","title":"0.7.0 - 2025-06-27","text":"<p>[!IMPORTANT] - Add MCP support.</p>"},{"location":"changelog/#features_5","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add MCP support in functions module - (224c408) - Belingud</li> <li>add --list-mcp option to main function - (4a536d4) - Belingud</li> <li>add MCP options to CLI entry point - (d7f3978) - Belingud</li> <li>add MCP support to LLMClient and providers - (58a4609) - Belingud</li> <li>add asyncio utilities and function wrapper - (fb9e02b) - Belingud</li> </ul>"},{"location":"changelog/#bug-fixes_2","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Add tool call chunk processing in AI21Provider - (df78c4f) - Belingud</li> </ul>"},{"location":"changelog/#refactor_2","title":"\ud83d\ude9c Refactor","text":"<ul> <li>simplify tool call creation and update default provider - (225e066) - Belingud</li> <li>remove unused imports and simplify DoubaoProvider constructor - (5effa9d) - Belingud</li> <li>update parse_choice_from_content to support ChoiceChunk - (7f2194f) - Belingud</li> <li>refactor tools module for MCP integration - (62ab778) - Belingud</li> </ul>"},{"location":"changelog/#documentation_8","title":"\ud83d\udcda Documentation","text":"<ul> <li>add default HF_PROVIDER to README - (98e0066) - Belingud</li> <li>add MCP support and documentation - (adfdd62) - Belingud</li> <li>update CHANGELOG.md for v0.6.4 release - (d16a66b) - Belingud</li> </ul>"},{"location":"changelog/#testing_3","title":"\ud83e\uddea Testing","text":"<ul> <li>update test case for new config key - (f38024c) - Belingud</li> <li>add tests for get_or_create_event_loop - (594b671) - Belingud</li> <li>update provider names and default value handling - (013a989) - Belingud</li> </ul>"},{"location":"changelog/#064-2025-06-24","title":"0.6.4 - 2025-06-24","text":""},{"location":"changelog/#features_6","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add new Cohere providers and improve type handling - (5d5f53c) - Belingud</li> <li>add HuggingFace provider to ProviderFactory - (a08cd70) - Belingud</li> <li>add HuggingFaceProvider class - (d81b42c) - Belingud</li> </ul>"},{"location":"changelog/#bug-fixes_3","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>handle empty content in CLI methods - (26ed101) - Belingud</li> </ul>"},{"location":"changelog/#refactor_3","title":"\ud83d\ude9c Refactor","text":"<ul> <li>standardize completion params keys in providers - (6802dfd) - Belingud</li> </ul>"},{"location":"changelog/#documentation_9","title":"\ud83d\udcda Documentation","text":"<ul> <li>update installation commands and supported providers - (9034c98) - Belingud</li> <li>add new models and configurations - (f2b02b8) - Belingud</li> <li>update CHANGELOG.md for v0.6.3 release - (8370366) - Belingud</li> </ul>"},{"location":"changelog/#testing_4","title":"\ud83e\uddea Testing","text":"<ul> <li>add tests for HuggingFaceProvider and update provider factory - (88f4a0b) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_4","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>add new dependencies and keywords - (f82a5b7) - Belingud</li> </ul>"},{"location":"changelog/#063-2025-06-23","title":"0.6.3 - 2025-06-23","text":""},{"location":"changelog/#features_7","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add error handling for better error show - (ced63b8) - Belingud</li> <li>add gemini and vertexai providers - (1cb56ad) - Belingud</li> </ul>"},{"location":"changelog/#bug-fixes_4","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>use display_stream return content for command execute - (5443822) - Belingud</li> <li>ensure non-empty config values are used in completion params - (c319ade) - Belingud</li> </ul>"},{"location":"changelog/#refactor_4","title":"\ud83d\ude9c Refactor","text":"<ul> <li>remove unused BaseProvider class - (1166114) - Belingud</li> </ul>"},{"location":"changelog/#documentation_10","title":"\ud83d\udcda Documentation","text":"<ul> <li>update CHANGELOG.md for v0.6.2 release - (b9e6354) - Belingud</li> </ul>"},{"location":"changelog/#testing_5","title":"\ud83e\uddea Testing","text":"<ul> <li>add tests for gemini and vertexai providers - (7a9f755) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_5","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update default reasoning effort to empty string - (e7f59c3) - Belingud</li> </ul>"},{"location":"changelog/#062-2025-06-23","title":"0.6.2 - 2025-06-23","text":""},{"location":"changelog/#features_8","title":"\u26f0\ufe0f  Features","text":"<ul> <li>update provider mappings in ProviderFactory - (3dce344) - Belingud</li> <li>add XaiProvider class based on OpenAIProvider - (f177da8) - Belingud</li> </ul>"},{"location":"changelog/#refactor_5","title":"\ud83d\ude9c Refactor","text":"<ul> <li>standardize completion parameter handling across providers - (25be368) - Belingud</li> </ul>"},{"location":"changelog/#documentation_11","title":"\ud83d\udcda Documentation","text":"<ul> <li>update default settings and add XAI provider - (1cef2f8) - Belingud</li> <li>update CHANGELOG.md for v0.6.1 release - (0c5ece3) - Belingud</li> </ul>"},{"location":"changelog/#testing_6","title":"\ud83e\uddea Testing","text":"<ul> <li>add xai to provider factory test - (f89068f) - Belingud</li> <li>add tests for XaiProvider and new Role class - (c77d070) - Belingud</li> <li>update provider referer and add new providers - (423cbbd) - Belingud</li> </ul>"},{"location":"changelog/#061-2025-06-22","title":"0.6.1 - 2025-06-22","text":""},{"location":"changelog/#features_9","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add error handling for empty choices in OpenAIProvider for better error message - (9cd2a28) - Belingud</li> <li>add minimax and targon providers - (e8df86f) - Belingud</li> </ul>"},{"location":"changelog/#refactor_6","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Unify completion params handling in LLM providers - (9132020) - Belingud</li> </ul>"},{"location":"changelog/#documentation_12","title":"\ud83d\udcda Documentation","text":"<ul> <li>update CHANGELOG.md for v0.6.0 release - (f9b9085) - Belingud</li> </ul>"},{"location":"changelog/#testing_7","title":"\ud83e\uddea Testing","text":"<ul> <li>update mocking strategy for provider tests - (4b9c410) - Belingud</li> </ul>"},{"location":"changelog/#060-2025-06-22","title":"0.6.0 - 2025-06-22","text":""},{"location":"changelog/#features_10","title":"\u26f0\ufe0f  Features","text":"<ul> <li>refactor OllamaProvider and update default temperature - (cf2dbe9) - Belingud</li> </ul>"},{"location":"changelog/#refactor_7","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Update provider naming and Groq N param handling - (309f70e) - Belingud</li> <li>revamp LLM client and provider architecture - (90f7d46) - Belingud</li> </ul>"},{"location":"changelog/#documentation_13","title":"\ud83d\udcda Documentation","text":"<ul> <li>update README with installation options and provider configs - (c7c088f) - Belingud</li> <li>update CHANGELOG.md for v0.5.9 release - (ba73961) - Belingud</li> </ul>"},{"location":"changelog/#testing_8","title":"\ud83e\uddea Testing","text":"<ul> <li>add llms package tests - (9c3e5c2) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_6","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>Update optional dependencies in pyproject.toml - (cc8f865) - Belingud</li> <li>Configure ruff linting and add 'all' optional deps - (3d660e3) - Belingud</li> <li>Update project keywords for broader AI model support - (5e09e7b) - Belingud</li> </ul>"},{"location":"changelog/#059-2025-06-12","title":"0.5.9 - 2025-06-12","text":""},{"location":"changelog/#documentation_14","title":"\ud83d\udcda Documentation","text":"<ul> <li>update CHANGELOG.md for v0.5.8 release - (18a47d2) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_7","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update click and typer dependencies - (1f99298) - Belingud</li> </ul>"},{"location":"changelog/#058-2025-05-22","title":"0.5.8 - 2025-05-22","text":""},{"location":"changelog/#refactor_8","title":"\ud83d\ude9c Refactor","text":"<ul> <li>move schemas imports and update StrEnum compatibility - (a8a5b3b) - Belingud</li> </ul>"},{"location":"changelog/#documentation_15","title":"\ud83d\udcda Documentation","text":"<ul> <li>update CHANGELOG.md for v0.5.7 release - (a0a641a) - Belingud</li> </ul>"},{"location":"changelog/#057-2025-05-22","title":"0.5.7 - 2025-05-22","text":""},{"location":"changelog/#features_11","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add support for EXTRA_HEADERS and EXTRA_BODY configuration - (7f8622f) - Belingud</li> </ul>"},{"location":"changelog/#documentation_16","title":"\ud83d\udcda Documentation","text":"<ul> <li>update README with new config options and dependencies - (ebe2023) - Belingud</li> <li>update CHANGELOG.md for v0.5.6 release - (2cf8e82) - Belingud</li> </ul>"},{"location":"changelog/#056-2025-05-22","title":"0.5.6 - 2025-05-22","text":""},{"location":"changelog/#bug-fixes_5","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>update project config and simplify printer logic - (029b7e2) - Belingud</li> </ul>"},{"location":"changelog/#documentation_17","title":"\ud83d\udcda Documentation","text":"<ul> <li>Fix formatting in README - (7e440da) - Belingud</li> <li>update CHANGELOG.md for v0.5.5 release - (5d12c69) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_8","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>add 'ai' as alternative command in pyproject.toml - (d1852f1) - Belingud</li> </ul>"},{"location":"changelog/#055-2025-05-21","title":"0.5.5 - 2025-05-21","text":""},{"location":"changelog/#refactor_9","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Improve case-insensitive command handling in CLI - (a9f1382) - Belingud</li> </ul>"},{"location":"changelog/#documentation_18","title":"\ud83d\udcda Documentation","text":"<ul> <li>add producthunt badge to README - (3b2d98b) - Belingud</li> <li>add example image for reasoning shell - (99ff117) - Belingud</li> <li>update CHANGELOG.md for v0.5.4 release - (fb5d95a) - Belingud</li> </ul>"},{"location":"changelog/#054-2025-05-14","title":"0.5.4 - 2025-05-14","text":""},{"location":"changelog/#documentation_19","title":"\ud83d\udcda Documentation","text":"<ul> <li>Update CHANGELOG.md for v0.5.3 release - (a0bf128) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_9","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>Update dependencies - (698a054) - Belingud</li> </ul>"},{"location":"changelog/#053-2025-05-14","title":"0.5.3 - 2025-05-14","text":""},{"location":"changelog/#documentation_20","title":"\ud83d\udcda Documentation","text":"<ul> <li>update CHANGELOG.md for v0.5.2 release - (4393332) - Belingud</li> </ul>"},{"location":"changelog/#052-2025-05-14","title":"0.5.2 - 2025-05-14","text":""},{"location":"changelog/#documentation_21","title":"\ud83d\udcda Documentation","text":"<ul> <li>Update sologan and logo - (1ccd4b5) - Belingud</li> <li>Update CHANGELOG.md for v0.5.1 release - (d7023dc) - Belingud</li> </ul>"},{"location":"changelog/#051-2025-05-14","title":"0.5.1 - 2025-05-14","text":""},{"location":"changelog/#refactor_10","title":"\ud83d\ude9c Refactor","text":"<ul> <li>remove unused command line argument - (2daf0e0) - Belingud</li> </ul>"},{"location":"changelog/#documentation_22","title":"\ud83d\udcda Documentation","text":"<ul> <li>add function call support to README - (191b5ae) - Belingud</li> <li>Update CHANGELOG.md for v0.5.0 release - (db7a328) - Belingud</li> </ul>"},{"location":"changelog/#050-2025-05-14","title":"0.5.0 - 2025-05-14","text":""},{"location":"changelog/#features_12","title":"\u26f0\ufe0f  Features","text":"<p>[!IMPORTANT] - Add Function Call support.</p> <ul> <li>Add reasoning effort configuration and function output panel - (f21d29b) - Belingud</li> <li>refactor client and printer logic, add function call support - (9f008be) - Belingud</li> <li>add non-stream completion function call - (572e2e2) - Belingud</li> <li>add non-stream completion function call - (13d6154) - Belingud</li> </ul>"},{"location":"changelog/#documentation_23","title":"\ud83d\udcda Documentation","text":"<ul> <li>Enhance README with function calling and provider info - (ab69e5c) - Belingud</li> <li>update changelog for v0.4.0 release - (66db3ee) - Belingud</li> </ul>"},{"location":"changelog/#testing_9","title":"\ud83e\uddea Testing","text":"<ul> <li>update test case for refactored project - (26e0faf) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_10","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update dependencies and script name - (562d930) - Belingud</li> </ul>"},{"location":"changelog/#040-2025-05-01","title":"0.4.0 - 2025-05-01","text":"<p>From this version, yaicli will use provider sdk instead of httpx raw request</p> <p>Providers: - OpenAI - Cohere</p>"},{"location":"changelog/#features_13","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add provider-based API client architecture with OpenAI and Cohere support - (9f68de1) - Belingud</li> <li>add role modify warning config and optimize lru_cache - (9c25162) - Belingud</li> <li>add role modification warnings and singleton pattern for RoleManager - (e2f9596) - Belingud</li> </ul>"},{"location":"changelog/#refactor_11","title":"\ud83d\ude9c Refactor","text":"<ul> <li>reorganize imports and improve code formatting - (c1ee133) - Belingud</li> </ul>"},{"location":"changelog/#documentation_24","title":"\ud83d\udcda Documentation","text":"<ul> <li>refine README with updated configurations and provider info - (90b8170) - Belingud</li> <li>update changelog for v0.3.3 release - (3c73873) - Belingud</li> </ul>"},{"location":"changelog/#testing_10","title":"\ud83e\uddea Testing","text":"<ul> <li>add provider tests for base, cohere, openai, and factory - (26e6ef2) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_11","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>add chat demo cast and use asciinema for example - (36f3dea) - Belingud</li> <li>add yaicli to isort formatting in Justfile - (d47def9) - Belingud</li> </ul>"},{"location":"changelog/#033-2025-04-29","title":"0.3.3 - 2025-04-29","text":""},{"location":"changelog/#refactor_12","title":"\ud83d\ude9c Refactor","text":"<ul> <li>remove redundant console print in CLI prompt logic - (1cf8241) - Belingud</li> </ul>"},{"location":"changelog/#documentation_25","title":"\ud83d\udcda Documentation","text":"<ul> <li>update changelog for v0.3.2 release - (328f51b) - Belingud</li> </ul>"},{"location":"changelog/#032-2025-04-29","title":"0.3.2 - 2025-04-29","text":""},{"location":"changelog/#bug-fixes_6","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>mode change in repl should also change role - (9dea555) - Belingud</li> </ul>"},{"location":"changelog/#documentation_26","title":"\ud83d\udcda Documentation","text":"<ul> <li>Update CHANGELOG.md for release 0.3.1 - (4511255) - Belingud</li> </ul>"},{"location":"changelog/#testing_11","title":"\ud83e\uddea Testing","text":"<ul> <li>add X-Title header to API client test - (f786c48) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_12","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>configure GitHub Actions for PyPI publishing - (b2d0cd4) - Belingud</li> </ul>"},{"location":"changelog/#031-2025-04-29","title":"0.3.1 - 2025-04-29","text":""},{"location":"changelog/#features_14","title":"\u26f0\ufe0f  Features","text":"<ul> <li>Add X-Title header to API requests - (e30060b) - Belingud</li> <li>clarify --code option help message - (74349c9) - Belingud</li> </ul>"},{"location":"changelog/#refactor_13","title":"\ud83d\ude9c Refactor","text":"<ul> <li>simplify chat manager test setup - (8650b05) - Belingud</li> </ul>"},{"location":"changelog/#documentation_27","title":"\ud83d\udcda Documentation","text":"<ul> <li>refine CLI documentation and add code example - (658ecf9) - Belingud</li> <li>Update README with new features and usage examples - (756942b) - Belingud</li> <li>Update 0.3.0 feat changelog - (34ad75a) - Belingud</li> <li>update CHANGELOG for v0.3.0 - (ce27418) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_13","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update Justfile commands and descriptions - (0c5b841) - Belingud</li> </ul>"},{"location":"changelog/#030-2025-04-28","title":"0.3.0 - 2025-04-28","text":"<ol> <li>Add roles create/delete/list/show commands</li> <li>Add role management functionality</li> <li>Improve configuration management</li> <li>Add reasoning show config, justify config</li> </ol>"},{"location":"changelog/#features_15","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add roles and improve configuration management - (fa883e9) - Belingud</li> </ul>"},{"location":"changelog/#refactor_14","title":"\ud83d\ude9c Refactor","text":"<ul> <li>reorder imports and remove unused imports - (28699dd) - Belingud</li> <li>Improve typing and streamline code for maintainability, add role funtion - (7f7230b) - Belingud</li> </ul>"},{"location":"changelog/#documentation_28","title":"\ud83d\udcda Documentation","text":"<ul> <li>improve documentation and configuration details - (28287d0) - Belingud</li> <li>Add reasoning example and update README - (c80da0f) - Belingud</li> <li>Update README.md - (1c169e5) - belingud</li> <li>update CHANGELOG.md for v0.2.0 release - (9cda1ea) - Belingud</li> </ul>"},{"location":"changelog/#testing_12","title":"\ud83e\uddea Testing","text":"<ul> <li>update CLI input handling and improve test cases for chat mode - (7a12f71) - Belingud</li> </ul>"},{"location":"changelog/#020-2025-04-22","title":"0.2.0 - 2025-04-22","text":""},{"location":"changelog/#features_16","title":"\u26f0\ufe0f  Features","text":"<ul> <li>enhance chat management and command-line interface - (6898273) - Belingud</li> <li>add chat persistent - (291d7b7) - Belingud</li> </ul>"},{"location":"changelog/#documentation_29","title":"\ud83d\udcda Documentation","text":"<ul> <li>improve documentation and add chat history features - (c0f1545) - Belingud</li> <li>Update README to clarify project description - (21d6389) - Belingud</li> <li>update CHANGELOG.md for v0.1.0 release - (474aaa4) - Belingud</li> </ul>"},{"location":"changelog/#testing_13","title":"\ud83e\uddea Testing","text":"<ul> <li>Update chat management and CLI tests - (1b3ccb0) - Belingud</li> <li>update test case for chat persistent - (af4f7e7) - Belingud</li> </ul>"},{"location":"changelog/#chroe","title":"Chroe","text":"<ul> <li>Add format command to Justfile - (676ba1b) - Belingud</li> </ul>"},{"location":"changelog/#010-2025-04-20","title":"0.1.0 - 2025-04-20","text":""},{"location":"changelog/#features_17","title":"\u26f0\ufe0f  Features","text":"<ul> <li>split content and reasoning content for better display - (83c7cff) - Belingud</li> </ul>"},{"location":"changelog/#refactor_15","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Replace \"reasoning\" with \"thinking\" in tests, fix CLI exit - (519d210) - Belingud</li> <li>Rename INTERACTIVE_MAX_HISTORY to INTERACTIVE_ROUND - (ac17cad) - Belingud</li> <li>refactor yaicli, imporve logic - (71d19d6) - Belingud</li> <li>extract completion response processing logic - (d9f8d08) - Belingud</li> <li>refactor yaicli structure - (e2ad1be) - Belingud</li> </ul>"},{"location":"changelog/#documentation_30","title":"\ud83d\udcda Documentation","text":"<ul> <li>improve README with detailed features and usage - (555f2b7) - Belingud</li> <li>add example artwork - (8b40706) - Belingud</li> <li>update README with timeout option and improved help text - (cd0f031) - Belingud</li> <li>update CHANGELOG for v0.0.19 release - (4ff2781) - Belingud</li> </ul>"},{"location":"changelog/#testing_14","title":"\ud83e\uddea Testing","text":"<ul> <li>add new options to mock CLI config - (0279155) - Belingud</li> <li>update test case for refactor - (d6072b3) - Belingud</li> <li>update tests for refactor - (024edb7) - Belingud</li> </ul>"},{"location":"changelog/#0019-2025-04-16","title":"0.0.19 - 2025-04-16","text":""},{"location":"changelog/#features_18","title":"\u26f0\ufe0f  Features","text":"<ul> <li>enhance configuration handling with type support and improve default config initialization - (34bc896) - Belingud</li> </ul>"},{"location":"changelog/#documentation_31","title":"\ud83d\udcda Documentation","text":"<ul> <li>update CHANGELOG.md for v0.0.18 release - (bc19008) - Belingud</li> </ul>"},{"location":"changelog/#0018-2025-04-14","title":"0.0.18 - 2025-04-14","text":""},{"location":"changelog/#features_19","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add auto-suggest from history and improve config handling - (22b31de) - Belingud</li> </ul>"},{"location":"changelog/#documentation_32","title":"\ud83d\udcda Documentation","text":"<ul> <li>update CHANGELOG.md for v0.0.17 release - (af27d29) - Belingud</li> </ul>"},{"location":"changelog/#0017-2025-04-14","title":"0.0.17 - 2025-04-14","text":""},{"location":"changelog/#bug-fixes_7","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Correct stdin TTY check and remove completer - (e2ee703) - Belingud</li> </ul>"},{"location":"changelog/#refactor_16","title":"\ud83d\ude9c Refactor","text":"<ul> <li>remove unused WordCompleter import - (968e6f1) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_14","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update yaicli version to 0.0.16 in uv.lock and CHANGELOG.md - (f9376c4) - Belingud</li> </ul>"},{"location":"changelog/#0016-2025-04-14","title":"0.0.16 - 2025-04-14","text":""},{"location":"changelog/#features_20","title":"\u26f0\ufe0f  Features","text":"<ul> <li>(cli) add max history limit and stdin support - (1b685df) - Belingud</li> </ul>"},{"location":"changelog/#refactor_17","title":"\ud83d\ude9c Refactor","text":"<ul> <li>Remove unused io import - (1e72f6c) - Belingud</li> </ul>"},{"location":"changelog/#documentation_33","title":"\ud83d\udcda Documentation","text":"<ul> <li>update README with new shortcuts and max history size - (af70000) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_15","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update yaicli version to 0.0.15 in uv.lock and CHANGELOG.md - (15faced) - Belingud</li> </ul>"},{"location":"changelog/#0015-2025-04-11","title":"0.0.15 - 2025-04-11","text":""},{"location":"changelog/#features_21","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add CODE_THEME configuration for code block styling - (ed0e8f6) - Belingud</li> </ul>"},{"location":"changelog/#bug-fixes_8","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>handle None value in SHELL environment variable - (a4e29f5) - Belingud</li> </ul>"},{"location":"changelog/#documentation_34","title":"\ud83d\udcda Documentation","text":"<ul> <li>update CHANGELOG.md with env key prefix change - (48b33da) - Belingud</li> <li>update README.md with improved command execution format - (03b2417) - Belingud</li> <li>update command execution UI in README.md - (a4aadc8) - Belingud</li> </ul>"},{"location":"changelog/#styling","title":"\ud83c\udfa8 Styling","text":"<ul> <li>fix spacing around 'or' operator in shell detection - (44cedc9) - Belingud</li> </ul>"},{"location":"changelog/#testing_15","title":"\ud83e\uddea Testing","text":"<ul> <li>add smoke tests for CLI and fix shell detection tests - (baa1beb) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_16","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update yaicli version to 0.0.14 in uv.lock and CHANGELOG.md - (9d21edf) - Belingud</li> </ul>"},{"location":"changelog/#0014-2025-04-10","title":"0.0.14 - 2025-04-10","text":""},{"location":"changelog/#features_22","title":"\u26f0\ufe0f  Features","text":"<ul> <li>(cli) enhance command execution and output display - (d101841) - Belingud</li> </ul>"},{"location":"changelog/#refactor_18","title":"\ud83d\ude9c Refactor","text":"<ul> <li>clean up unused imports in yaicli.py - (71c64af) - Belingud</li> </ul>"},{"location":"changelog/#documentation_35","title":"\ud83d\udcda Documentation","text":"<ul> <li>(Justfile) update changelog command message - (b060812) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_17","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update yaicli version to 0.0.13 in uv.lock and CHANGELOG.md - (3806f70) - Belingud</li> </ul>"},{"location":"changelog/#0013-2025-04-09","title":"0.0.13 - 2025-04-09","text":"<p>[!IMPORTANT] - Change env key prefix from AI_ to YAI_</p>"},{"location":"changelog/#features_23","title":"\u26f0\ufe0f  Features","text":"<ul> <li>(cli) add command edit option and improve console output styling - (48c14cc) - Belingud</li> <li>refactor CLI and add new configuration options - (ad09301) - Belingud</li> </ul>"},{"location":"changelog/#documentation_36","title":"\ud83d\udcda Documentation","text":"<ul> <li>update README with new ASCII art and command examples - (cdcb9af) - Belingud</li> <li>update CHANGELOG.md for v0.0.12 release - (456c454) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_18","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>add ruff cache cleanup and uv lock to changelog - (218b310) - Belingud</li> </ul>"},{"location":"changelog/#0012-2025-04-08","title":"0.0.12 - 2025-04-08","text":""},{"location":"changelog/#features_24","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add chat history and improve CLI UX with completions - (7749883) - Belingud</li> </ul>"},{"location":"changelog/#documentation_37","title":"\ud83d\udcda Documentation","text":"<ul> <li>restructure CLI options documentation in README - (142ac78) - Belingud</li> <li>update CHANGELOG.md for v0.0.11 release - (6a6b3ae) - Belingud</li> </ul>"},{"location":"changelog/#0011-2025-04-07","title":"0.0.11 - 2025-04-07","text":""},{"location":"changelog/#refactor_19","title":"\ud83d\ude9c Refactor","text":"<ul> <li>(yaicli) improve stream response parsing and add plaintext test - (92383ae) - Belingud</li> <li>replace requests with httpx for HTTP client and update dependencies - (cfc3823) - Belingud</li> </ul>"},{"location":"changelog/#documentation_38","title":"\ud83d\udcda Documentation","text":"<ul> <li>update project title in README - (cf8aaa7) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_19","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>add publish command to Justfile - (a2a76e5) - Belingud</li> <li>update yaicli version to 0.0.10 and CHANGELOG - (aab8fb2) - Belingud</li> </ul>"},{"location":"changelog/#0010-2025-04-05","title":"0.0.10 - 2025-04-05","text":""},{"location":"changelog/#documentation_39","title":"\ud83d\udcda Documentation","text":"<ul> <li>update assistant name in default prompt - (e92313e) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_20","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update yaicli version to 0.0.9 and CHANGELOG - (2a47a8b) - Belingud</li> </ul>"},{"location":"changelog/#009-2025-04-05","title":"0.0.9 - 2025-04-05","text":""},{"location":"changelog/#features_25","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add reasoning model support - (8a3d21b) - Belingud</li> </ul>"},{"location":"changelog/#documentation_40","title":"\ud83d\udcda Documentation","text":"<ul> <li>add configuration guide for non-OpenAI LLM providers - (20f590d) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_21","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update yaicli version to 0.0.8 and improve README - (282d3b5) - Belingud</li> </ul>"},{"location":"changelog/#008-2025-04-04","title":"0.0.8 - 2025-04-04","text":""},{"location":"changelog/#features_26","title":"\u26f0\ufe0f  Features","text":"<ul> <li>add command filtering and improve history management - (e743b13) - Belingud</li> </ul>"},{"location":"changelog/#testing_16","title":"\ud83e\uddea Testing","text":"<ul> <li>restructure and add new test files for CLI functionality - (09c81eb) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_22","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update yaicli version to 0.0.7 and improve README - (71a8eff) - Belingud</li> </ul>"},{"location":"changelog/#007-2025-04-04","title":"0.0.7 - 2025-04-04","text":""},{"location":"changelog/#refactor_20","title":"\ud83d\ude9c Refactor","text":"<ul> <li>simplify CLI structure and improve code maintainability - (4eed7d8) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_23","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update yaicli version to 0.0.6 and CHANGELOG - (563a468) - Belingud</li> </ul>"},{"location":"changelog/#006-2025-04-03","title":"0.0.6 - 2025-04-03","text":""},{"location":"changelog/#refactor_21","title":"\ud83d\ude9c Refactor","text":"<ul> <li>update environment variable names and remove max_tokens - (3abba5f) - Belingud</li> <li>simplify config handling and remove DEFAULT_MODE - (3439eda) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_24","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update CHANGELOG and uv.lock for version 0.0.5 - (d5abef4) - Belingud</li> </ul>"},{"location":"changelog/#005-2025-04-02","title":"0.0.5 - 2025-04-02","text":""},{"location":"changelog/#refactor_22","title":"\ud83d\ude9c Refactor","text":"<ul> <li>rename ShellAI to YAICLI and update related tests - (2284297) - Belingud</li> </ul>"},{"location":"changelog/#documentation_41","title":"\ud83d\udcda Documentation","text":"<ul> <li>update README with detailed project information and usage instructions - (c71d50c) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_25","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update yaicli version to 0.0.4 - (03cdedb) - Belingud</li> </ul>"},{"location":"changelog/#004-2025-04-02","title":"0.0.4 - 2025-04-02","text":""},{"location":"changelog/#refactor_23","title":"\ud83d\ude9c Refactor","text":"<ul> <li>simplify multi-line statements for better readability - (4870bb7) - Belingud</li> <li>rename ShellAI to yaicli and improve CLI help handling - (dad0905) - Belingud</li> </ul>"},{"location":"changelog/#miscellaneous-tasks_26","title":"\u2699\ufe0f Miscellaneous Tasks","text":"<ul> <li>update yaicli version to 0.0.3 - (a689a02) - Belingud</li> </ul>"},{"location":"changelog/#003-2025-04-02","title":"0.0.3 - 2025-04-02","text":""},{"location":"changelog/#refactor_24","title":"\ud83d\ude9c Refactor","text":"<ul> <li>(changelog) switch from git log to git cliff for changelog generation - (242a51d) - Belingud</li> <li>update CLI assistant description and error handling - (1a8bcdc) - Belingud</li> <li>fix spacing in clean target and simplify build target - (2a1050e) - Belingud</li> </ul>"},{"location":"changelog/#002-2025-04-02","title":"[0.0.2] - 2025-04-02","text":""},{"location":"changelog/#features_27","title":"\u26f0\ufe0f  Features","text":"<ul> <li>enhance OS detection and LLM API request handling - (8c00de0) - Belingud</li> <li>add new dependencies and configurable API paths - (3be3f67) - Belingud</li> </ul>"},{"location":"changelog/#refactor_25","title":"\ud83d\ude9c Refactor","text":"<ul> <li>(config) migrate config from JSON to INI format and enhance error handling - (f556872) - Belingud</li> <li>update config path to reflect project name change - (e6bf761) - Belingud</li> <li>rename project from llmcli to yaicli and update related files - (cdb5a97) - Belingud</li> <li>rename project from shellai to llmcli and update related files - (db4ecb8) - Belingud</li> <li>reorganize imports and improve code readability - (0f52c05) - Belingud</li> <li>migrate llmcli to class-based ShellAI implementation - (2509cba) - Belingud</li> </ul>"},{"location":"changelog/#build","title":"Build","text":"<ul> <li>add bump2version for version management - (de287f1) - Belingud</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions to YAICLI are welcome! This page outlines how you can help improve the project.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<p>There are several ways to contribute to YAICLI:</p> <ul> <li>Bug Reports: Open an issue describing the bug and how to reproduce it</li> <li>Feature Requests: Suggest new features or improvements</li> <li>Code Contributions: Submit a pull request with your changes</li> <li>Documentation: Help improve or translate the documentation</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>uv, the Python project manager (https://github.com/astral-sh/uv)</li> <li>Git</li> <li>A GitHub account</li> </ul>"},{"location":"contributing/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/YOUR-USERNAME/yaicli.git\ncd yaicli\n</code></pre></li> <li>Create a virtual environment:    <pre><code>uv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre></li> <li>Install dependencies for development:    <pre><code>uv sync --all-extras\n</code></pre></li> </ol>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<p>YAICLI follows these style guidelines:</p> <ul> <li>Use Ruff for code formatting</li> <li>Follow PEP 8 style guidelines</li> <li>Write docstrings for all public functions, classes, and methods</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<p>Before submitting a PR, make sure to run the tests:</p> <pre><code>pytest\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a new branch for your feature or bugfix</li> <li>Make your changes</li> <li>Run the tests to ensure everything works</li> <li>Update the documentation if necessary</li> <li>Submit a pull request describing your changes</li> </ol>"},{"location":"contributing/#adding-a-new-provider","title":"Adding a New Provider","text":"<p>To add a new LLM provider to YAICLI:</p> <ol> <li>Create a new file in <code>yaicli/llms/providers/</code> named after your provider (e.g., <code>my_provider.py</code>)</li> <li>Implement the provider class following the interface pattern of existing providers</li> <li>Add the provider to the provider registry in <code>yaicli/llms/providers/__init__.py</code></li> <li>Update any configuration-related code to recognize the new provider</li> <li>Add tests for your provider in the <code>tests/llms/</code> directory</li> </ol>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>When contributing documentation:</p> <ol> <li>Follow the existing structure</li> <li>Use Markdown for all documentation files</li> <li>Test changes locally using MkDocs:    <pre><code>mkdocs serve\n</code></pre></li> </ol>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to YAICLI, you agree that your contributions will be licensed under the project's Apache License 2.0. </p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>This page addresses common questions and issues encountered when using YAICLI.</p>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-yaicli","title":"What is YAICLI?","text":"<p>YAICLI is a command-line interface for interacting with Large Language Models (LLMs). It allows you to chat with AI assistants, generate shell commands, and get quick answers directly from your terminal.</p>"},{"location":"faq/#what-llm-providers-are-supported","title":"What LLM providers are supported?","text":"<p>YAICLI supports multiple providers including: - OpenAI (GPT-4o, GPT-3.5, etc.) - Google Gemini - Claude (Anthropic) - Cohere - Mistral - Ollama - Many others (see Providers Overview)</p>"},{"location":"faq/#is-an-internet-connection-required","title":"Is an internet connection required?","text":"<p>Yes, for most providers. YAICLI communicates with LLM APIs that require internet access. However, you can use local models with providers like Ollama.</p>"},{"location":"faq/#how-is-my-data-handled","title":"How is my data handled?","text":"<p>YAICLI stores your conversation history locally. Your prompts and API responses are sent to the LLM provider you configure, according to their privacy policies. No data is sent to YAICLI developers.</p>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"faq/#why-am-i-getting-command-not-found-after-installing","title":"Why am I getting \"command not found\" after installing?","text":"<p>Make sure your Python scripts directory is in your PATH. Try these solutions:</p> <pre><code># Option 1: Find the installation location\nwhich ai\n\n# Option 2: Add to PATH (add to .bashrc or .zshrc)\nexport PATH=\"$HOME/.local/bin:$PATH\"\n</code></pre>"},{"location":"faq/#how-do-i-set-up-my-api-key","title":"How do I set up my API key?","text":"<p>You can set your API key in three ways: 1. In the config file (<code>~/.config/yaicli/config.ini</code>) 2. As an environment variable (<code>export YAI_API_KEY=\"your-key\"</code>) 3. Using a .env file in your project directory</p>"},{"location":"faq/#can-i-use-multiple-api-keys-for-different-providers","title":"Can I use multiple API keys for different providers?","text":"<p>Yes! You can create separate configuration sections or switch providers using environment variables:</p> <pre><code># Use OpenAI\nexport YAI_PROVIDER=openai\nexport YAI_API_KEY=sk-...\n\n# Use Gemini (in another session or after changing vars)\nexport YAI_PROVIDER=gemini\nexport YAI_API_KEY=AI...\n</code></pre>"},{"location":"faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/#how-do-i-save-a-chat-session","title":"How do I save a chat session?","text":"<p>In chat mode, use the <code>/save</code> command followed by a title: <pre><code>\ud83d\udcac &gt; /save \"Python Tips\"\n</code></pre></p> <p>Or start a named session that will be automatically saved: <pre><code>ai --chat \"Python Tips\"\n</code></pre></p>"},{"location":"faq/#how-do-i-control-output-formatting","title":"How do I control output formatting?","text":"<p>Use the <code>--justify</code> option to control text alignment: <pre><code>ai --justify center \"Write a poem about AI\"\n</code></pre></p> <p>Options include: <code>default</code>, <code>left</code>, <code>center</code>, <code>right</code>, and <code>full</code>.</p>"},{"location":"faq/#how-do-i-disable-streaming-responses","title":"How do I disable streaming responses?","text":"<p>Use the <code>--no-stream</code> flag or set <code>STREAM=false</code> in your config: <pre><code>ai --no-stream \"What is the capital of France?\"\n</code></pre></p>"},{"location":"faq/#how-do-i-change-the-code-highlighting-theme","title":"How do I change the code highlighting theme?","text":"<p>Set the <code>CODE_THEME</code> in your config file: <pre><code>CODE_THEME = monokai  # or any other Pygments theme\n</code></pre></p>"},{"location":"faq/#how-do-i-execute-shell-commands-safely","title":"How do I execute shell commands safely?","text":"<p>Use shell mode with the <code>-s</code> or <code>--shell</code> flag: <pre><code>ai -s \"Find all PNG files in the current directory\"\n</code></pre> You'll be prompted to review and optionally edit commands before execution.</p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#why-am-i-getting-api-errors","title":"Why am I getting API errors?","text":"<p>Common causes: - Invalid API key - Rate limits or quota exceeded - Network connectivity issues - Incompatible model for your provider</p> <p>Usually you will see a error message in output like <code>429 too many requests</code>.</p> <p>Try running with the verbose flag to see detailed error information: <pre><code>ai --verbose \"Test query\"\n</code></pre></p>"},{"location":"faq/#how-do-i-fix-ssl-certificate-verification-failed","title":"How do I fix \"SSL Certificate Verification Failed\"?","text":"<p>This typically happens in corporate environments with SSL inspection. Try:</p> <pre><code># Not recommended for security reasons, but may be necessary\nexport PYTHONWARNINGS=\"ignore:Unverified HTTPS request\"\nexport REQUESTS_CA_BUNDLE=/path/to/corporate/certificate.pem\n</code></pre>"},{"location":"faq/#how-do-i-update-yaicli","title":"How do I update YAICLI?","text":"<pre><code># Using pip\npip install --upgrade yaicli\n\n# Using pipx\npipx upgrade yaicli\n\n# Using uv\nuv tool upgrade yaicli\n</code></pre>"},{"location":"faq/#why-are-my-non-ascii-characters-displaying-incorrectly","title":"Why are my non-ASCII characters displaying incorrectly?","text":"<p>Make sure your terminal supports UTF-8 and has a compatible font. On Windows, you might need to: <pre><code>chcp 65001  # Set console code page to UTF-8\n</code></pre></p>"},{"location":"faq/#advanced-questions","title":"Advanced Questions","text":""},{"location":"faq/#how-do-i-create-custom-roles","title":"How do I create custom roles?","text":"<p>Create a new role: <pre><code>ai --create-role \"SQL Expert\"\n</code></pre></p> <p>Then use it: <pre><code>ai --role \"SQL Expert\" \"Optimize this query\"\n</code></pre></p>"},{"location":"faq/#can-i-use-function-calling","title":"Can I use function calling?","text":"<p>Yes, enable function calling with: <pre><code>ai --enable-functions \"Check the weather in New York\"\n</code></pre></p> <p>Install default functions: <pre><code>ai --install-functions\n</code></pre></p>"},{"location":"faq/#how-do-i-create-custom-functions","title":"How do I create custom functions?","text":"<p>Add your function definitions to: <pre><code>~/.config/yaicli/functions/\n</code></pre></p> <p>See the Usage Configuration Guide for details on function configuration.</p>"},{"location":"faq/#how-do-i-use-mcp-multi-provider-chain-protocol","title":"How do I use MCP (Multi-provider Chain Protocol)?","text":"<p>Enable MCP with: <pre><code>ai --enable-mcp \"Search for the latest AI research papers\"\n</code></pre></p> <p>Configure MCP in: <pre><code>~/.config/yaicli/mcp.json\n</code></pre></p>"},{"location":"faq/#contributing","title":"Contributing","text":""},{"location":"faq/#how-can-i-contribute-to-yaicli","title":"How can I contribute to YAICLI?","text":"<p>We welcome contributions! See the contributing guidelines for details on: - Reporting bugs - Suggesting features - Submitting pull requests - Code standards</p>"},{"location":"faq/#where-can-i-report-bugs-or-request-features","title":"Where can I report bugs or request features?","text":"<p>Please submit issues on the GitHub repository.</p>"},{"location":"getting-started/","title":"Getting Started with YAICLI","text":"<p>YAICLI is a powerful yet lightweight command-line AI assistant that brings the capabilities of Large Language Models (LLMs) like GPT-4o directly to your terminal. This guide will help you get up and running quickly.</p>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":"<p>After installation, you can start using YAICLI immediately:</p> <pre><code># Get a quick answer\nai \"What is the capital of France?\"\n\n# Start an interactive chat session\nai --chat\n\n# Generate and execute shell commands\nai --shell \"Create a backup of my Documents folder\"\n\n# Generate code snippets (default is Python)\nai --code \"Write a function to sort a list\"\n</code></pre>"},{"location":"getting-started/#first-time-setup","title":"First-time Setup","text":"<ol> <li>Run <code>ai</code> once to generate the default configuration file</li> <li>Edit <code>~/.config/yaicli/config.ini</code> to add your API key</li> <li>Customize other settings as needed</li> </ol>"},{"location":"getting-started/#configuration","title":"Configuration","text":"<p>YAICLI uses a layered configuration approach: - Environment variables (highest priority) - Config file (<code>~/.config/yaicli/config.ini</code>) - Sensible defaults (lowest priority)</p> <p>You can view the default configuration template with:</p> <pre><code>ai --template\n</code></pre>"},{"location":"getting-started/#basic-usage-patterns","title":"Basic Usage Patterns","text":""},{"location":"getting-started/#direct-queries","title":"Direct Queries","text":"<p>Get quick answers without entering interactive mode:</p> <pre><code>ai \"What is quantum computing?\"\n</code></pre>"},{"location":"getting-started/#interactive-chat-mode","title":"Interactive Chat Mode","text":"<p>Start a persistent conversation with context tracking:</p> <pre><code>ai --chat \"Python programming\"\n</code></pre>"},{"location":"getting-started/#command-generation","title":"Command Generation","text":"<p>Get AI to generate and optionally execute shell commands:</p> <pre><code>ai --shell \"Find all large files in my home directory\"\n</code></pre>"},{"location":"getting-started/#code-generation","title":"Code Generation","text":"<p>Generate code snippets with syntax highlighting:</p> <pre><code>ai --code \"Create a web scraper for news headlines\"\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Explore detailed usage documentation</li> <li>Learn about configuration options</li> <li>Check out available LLM providers</li> <li>Discover advanced features</li> </ul>"},{"location":"install/","title":"Installation Guide","text":"<p>YAICLI can be installed using several methods. Choose the one that best suits your needs.</p>"},{"location":"install/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> </ul>"},{"location":"install/#installation-methods","title":"Installation Methods","text":""},{"location":"install/#using-pip-recommended","title":"Using pip (Recommended)","text":"<p>The simplest way to install YAICLI is using pip:</p> <pre><code>pip install yaicli\n</code></pre>"},{"location":"install/#using-pipx-isolated-environment","title":"Using pipx (Isolated Environment)","text":"<p>If you prefer to install the tool in an isolated environment:</p> <pre><code>pipx install yaicli\n</code></pre>"},{"location":"install/#using-uv-faster-installation","title":"Using uv (Faster Installation)","text":"<p>For faster installation with the uv package manager:</p> <pre><code>uv tool install yaicli\n</code></pre>"},{"location":"install/#installing-with-optional-dependencies","title":"Installing with Optional Dependencies","text":"<p>YAICLI offers several optional dependency groups that you can install based on your needs:</p>"},{"location":"install/#all-dependencies","title":"All Dependencies","text":"<pre><code># Using pip\npip install 'yaicli[all]'\n\n# Using uv\nuv tool install 'yaicli[all]'\n</code></pre>"},{"location":"install/#specific-provider-support","title":"Specific Provider Support","text":"<pre><code># Using pip\npip install 'yaicli[ollama,cohere,doubao,huggingface,gemini,mistral]'\n\n# Using uv\nuv tool install 'yaicli[ollama,cohere,doubao,huggingface,gemini,mistral]'\n</code></pre>"},{"location":"install/#installing-from-source","title":"Installing from Source","text":"<p>For the latest development version:</p> <pre><code>git clone https://github.com/belingud/yaicli.git\ncd yaicli\npip install .\n</code></pre>"},{"location":"install/#verifying-installation","title":"Verifying Installation","text":"<p>To verify that YAICLI was installed correctly:</p> <pre><code>ai --help\n</code></pre> <p>This should display the help information for YAICLI.</p>"},{"location":"install/#first-time-setup","title":"First-Time Setup","text":"<p>After installation, run the tool once to generate the default configuration file:</p> <pre><code>ai\n</code></pre> <p>Then edit the configuration file located at <code>~/.config/yaicli/config.ini</code> to add your API key and customize settings:</p> <pre><code># Open with your preferred editor\nnano ~/.config/yaicli/config.ini\n</code></pre>"},{"location":"install/#next-steps","title":"Next Steps","text":"<ul> <li>Learn how to get started with YAICLI</li> <li>Explore configuration options</li> <li>Check out available providers</li> </ul>"},{"location":"advanced/debugging/","title":"Debugging","text":"<p>TODO: Add content for debugging.</p>"},{"location":"advanced/history/","title":"Conversation &amp; History Management","text":"<p>YAICLI provides robust conversation history management features to help you maintain context and revisit past interactions. This page explains how the history system works and how to manage your chat history.</p>"},{"location":"advanced/history/#command-history","title":"Command History","text":"<p>YAICLI maintains a history of your commands and interactions, allowing you to: - Search through previous commands - Reuse past queries - Keep track of your AI conversations</p>"},{"location":"advanced/history/#history-storage","title":"History Storage","text":"<p>By default, command history is stored in: - Linux/macOS: <code>~/.yaicli_history</code> - Windows: <code>%USERPROFILE%\\.yaicli_history</code></p> <p>The history file uses a simple text format with each command on a new line.</p>"},{"location":"advanced/history/#history-size","title":"History Size","text":"<p>You can configure the maximum number of history entries to retain:</p> <pre><code>[core]\nMAX_HISTORY=500\n</code></pre> <p>Or using an environment variable:</p> <pre><code>export YAI_MAX_HISTORY=1000\n</code></pre>"},{"location":"advanced/history/#viewing-history","title":"Viewing History","text":"<p>In interactive mode, use the <code>/his</code> command to view your command history:</p> <pre><code>\ud83d\udcac &gt; /his\nChat History:\n1 User: What is quantum computing?\n    Assistant: Quantum computing is a type of computation...\n2 User: What are qubits?\n    Assistant: Qubits (quantum bits) are the basic...\n</code></pre>"},{"location":"advanced/history/#searching-history","title":"Searching History","text":"<p>Use <code>Ctrl+R</code> to search through your command history:</p> <ol> <li>Press <code>Ctrl+R</code></li> <li>Type part of a previous command</li> <li>Press <code>Ctrl+R</code> again to cycle through matching results</li> <li>Press <code>Enter</code> to select the current match</li> </ol>"},{"location":"advanced/history/#auto-suggestion","title":"Auto-Suggestion","text":"<p>YAICLI can suggest commands based on your history as you type. Enable this feature in your config:</p> <pre><code>[core]\nAUTO_SUGGEST=true\n</code></pre> <p>When enabled, YAICLI will show suggestions in light gray text as you type. Press the right arrow key to accept a suggestion.</p>"},{"location":"advanced/history/#chat-sessions","title":"Chat Sessions","text":""},{"location":"advanced/history/#chat-persistence","title":"Chat Persistence","text":"<p>Chat sessions can be persistent or temporary:</p> <ul> <li>Persistent Session: Automatically saved with a title</li> <li>Temporary Session: Not saved unless explicitly requested</li> </ul> <p>To start a persistent chat session:</p> <pre><code>ai --chat \"Python programming\"\n</code></pre> <p>To convert a temporary session to persistent:</p> <pre><code>\ud83d\udcac &gt; /save \"Python programming\"\nChat saved as: Python programming\n</code></pre>"},{"location":"advanced/history/#chat-storage-location","title":"Chat Storage Location","text":"<p>Chat history is stored in: - Default: A temporary directory specific to your system - Configurable through <code>CHAT_HISTORY_DIR</code> setting</p> <pre><code>[core]\nCHAT_HISTORY_DIR=/path/to/custom/directory\nMAX_SAVED_CHATS=20\n</code></pre>"},{"location":"advanced/history/#listing-saved-chats","title":"Listing Saved Chats","text":"<p>To list your saved chat sessions:</p> <pre><code># From command line\nai --list-chats\n\n# From interactive mode\n\ud83d\udcac &gt; /list\n</code></pre> <p>This displays a numbered list of saved chats with their creation dates.</p>"},{"location":"advanced/history/#loading-saved-chats","title":"Loading Saved Chats","text":"<p>You can load saved chats in two ways:</p> <pre><code># From command line (using the chat title)\nai --chat \"Python programming\"\n\n# From interactive mode (using the chat index from /list)\n\ud83d\udcac &gt; /load 3\n</code></pre>"},{"location":"advanced/history/#deleting-saved-chats","title":"Deleting Saved Chats","text":"<p>To delete a saved chat:</p> <pre><code># From interactive mode\n\ud83d\udcac &gt; /del 3\n</code></pre> <p>This will permanently remove the selected chat session.</p>"},{"location":"advanced/history/#context-management","title":"Context Management","text":""},{"location":"advanced/history/#context-window","title":"Context Window","text":"<p>YAICLI automatically manages the conversation context based on: - The model's context window limitations - The number of messages in the current session - The complexity and length of each message</p>"},{"location":"advanced/history/#clearing-context","title":"Clearing Context","text":"<p>To clear the current conversation context:</p> <pre><code>\ud83d\udcac &gt; /clear\n</code></pre> <p>This removes all previous messages from the current session's context but does not delete saved chat history.</p>"},{"location":"advanced/history/#advanced-history-features","title":"Advanced History Features","text":""},{"location":"advanced/history/#history-file-format","title":"History File Format","text":"<p>The chat history files are stored as JSON with this structure:</p> <pre><code>{\n  \"title\": \"Chat Title\",\n  \"created_at\": \"2024-06-01T12:00:00Z\",\n  \"updated_at\": \"2024-06-01T13:30:00Z\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"User message here\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Assistant response here\"\n    }\n  ]\n}\n</code></pre>"},{"location":"advanced/history/#manual-history-editing","title":"Manual History Editing","text":"<p>You can manually edit the history files:</p> <ol> <li>Navigate to your chat history directory</li> <li>Open the JSON file with any text editor</li> <li>Modify as needed (keeping the JSON structure intact)</li> <li>Save the file</li> </ol>"},{"location":"advanced/history/#history-migration","title":"History Migration","text":"<p>When upgrading YAICLI, history files are automatically migrated to newer formats if needed. If you're experiencing issues with history after an upgrade, try:</p> <pre><code># Backup your history\ncp ~/.yaicli_history ~/.yaicli_history.bak\n\n# Remove the history file to create a fresh one\nrm ~/.yaicli_history\n</code></pre>"},{"location":"advanced/history/#disabling-history","title":"Disabling History","text":"<p>You can disable command history saving completely by setting:</p> <pre><code>[core]\nMAX_HISTORY=0\n</code></pre>"},{"location":"advanced/history/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Cleaning: Periodically review and delete unnecessary chat sessions</li> <li>Meaningful Titles: Use descriptive titles for your persistent chats</li> <li>Context Awareness: Clear context when switching topics within the same session</li> <li>Backup: Occasionally backup your chat history directory for important conversations</li> </ol>"},{"location":"advanced/history/#next-steps","title":"Next Steps","text":"<ul> <li>Explore debugging and logging</li> <li>Check out prompt templates</li> </ul>"},{"location":"advanced/prompts/","title":"Prompt Template System","text":"<p>YAICLI features a powerful and flexible prompt template system that allows you to define custom roles and behaviors for your AI assistant. This page explains how to use and customize the prompt system.</p>"},{"location":"advanced/prompts/#understanding-roles","title":"Understanding Roles","text":"<p>In YAICLI, a \"role\" is a predefined set of instructions, preferences, and behaviors assigned to the AI. Roles help guide how the AI responds to your queries.</p>"},{"location":"advanced/prompts/#default-roles","title":"Default Roles","text":"<p>YAICLI comes with a built-in <code>DEFAULT</code> role that provides a solid foundation for general-purpose interactions. This role instructs the AI to be helpful, accurate, and concise.</p>"},{"location":"advanced/prompts/#creating-custom-roles","title":"Creating Custom Roles","text":"<p>You can create custom roles to tailor the AI's behavior for specific tasks or domains:</p> <pre><code># Create a new role through interactive prompting\nai --create-role \"SQL Expert\"\n\n# You'll be asked to provide a description for this role\n</code></pre> <p>When creating a role, you'll be prompted to describe how the AI should behave in that role. For example:</p> <pre><code>Please provide a description for the SQL Expert role:\n\nYou are an expert SQL database engineer with deep knowledge of database \noptimization, query design, and best practices across multiple SQL dialects \nincluding MySQL, PostgreSQL, SQLite, and Microsoft SQL Server. \n\nWhen asked about SQL, you should:\n1. Consider the specific dialect if mentioned, or clarify which dialect you're using\n2. Provide optimized queries with explanations of your optimization choices\n3. Consider indexing, query planning, and performance implications\n4. Format SQL with proper indentation and capitalization of keywords\n</code></pre>"},{"location":"advanced/prompts/#using-custom-roles","title":"Using Custom Roles","text":"<p>Once created, you can use your custom roles:</p> <pre><code># Use in direct query mode\nai --role \"SQL Expert\" \"Optimize this query: SELECT * FROM users WHERE active = true\"\n\n# Use in chat mode\nai --chat --role \"SQL Expert\"\n\n# Use in shell command mode\nai --shell --role \"DevOps Engineer\" \"Set up a cronjob\"\n</code></pre>"},{"location":"advanced/prompts/#managing-roles","title":"Managing Roles","text":"<p>YAICLI provides commands to manage your roles:</p> <pre><code># List all available roles\nai --list-roles\n\n# View a specific role's description\nai --show-role \"SQL Expert\"\n\n# Delete a role\nai --delete-role \"SQL Expert\"\n</code></pre>"},{"location":"advanced/prompts/#role-storage","title":"Role Storage","text":"<p>Custom roles are stored in: - Linux/macOS: <code>~/.config/yaicli/roles/</code> - Windows: <code>C:\\Users\\&lt;username&gt;\\.config\\yaicli\\roles\\</code></p> <p>Each role is saved as a JSON file with the role name.</p>"},{"location":"advanced/prompts/#setting-a-default-role","title":"Setting a Default Role","text":"<p>You can set a default role in your configuration file:</p> <pre><code>[core]\nDEFAULT_ROLE=SQL Expert\n</code></pre> <p>Or using an environment variable:</p> <pre><code>export YAI_DEFAULT_ROLE=\"SQL Expert\"\n</code></pre>"},{"location":"advanced/prompts/#advanced-role-creation","title":"Advanced Role Creation","text":""},{"location":"advanced/prompts/#role-structure","title":"Role Structure","text":"<p>Roles are defined as JSON files with the following structure:</p> <pre><code>{\n  \"name\": \"Role Name\",\n  \"description\": \"Detailed instructions for the AI...\",\n  \"created_at\": \"2024-06-01T12:00:00Z\",\n  \"updated_at\": \"2024-06-01T12:00:00Z\"\n}\n</code></pre>"},{"location":"advanced/prompts/#creating-roles-manually","title":"Creating Roles Manually","text":"<p>You can create roles manually by adding JSON files to the roles directory:</p> <ol> <li>Create a file named <code>role_name.json</code> in the roles directory</li> <li>Add the role structure as shown above</li> <li>Restart YAICLI or run <code>ai --list-roles</code> to see your new role</li> </ol>"},{"location":"advanced/prompts/#role-modification-warning","title":"Role Modification Warning","text":"<p>By default, YAICLI will warn you when attempting to modify built-in roles. To disable this warning:</p> <pre><code>[core]\nROLE_MODIFY_WARNING=false\n</code></pre>"},{"location":"advanced/prompts/#best-practices-for-writing-role-descriptions","title":"Best Practices for Writing Role Descriptions","text":"<p>For the most effective custom roles:</p> <ol> <li>Be Specific: Clearly define the expertise and focus area</li> <li>Set Boundaries: Specify what the AI should and should not address</li> <li>Format Guidelines: Include preferred output formats, styles, or conventions</li> <li>Examples: When helpful, include examples of desired responses</li> <li>Context: Add relevant domain knowledge or context that might help the AI</li> </ol>"},{"location":"advanced/prompts/#role-examples","title":"Role Examples","text":""},{"location":"advanced/prompts/#technical-writer","title":"Technical Writer","text":"<pre><code>You are a technical writer specializing in creating clear, concise documentation.\n\nWhen responding:\n1. Use plain language and avoid jargon unless necessary\n2. Structure content with logical headings and lists\n3. Focus on practical examples and use cases\n4. Include both beginner and advanced perspectives\n5. Use consistent terminology throughout\n\nFor code examples, include comments explaining key concepts.\n</code></pre>"},{"location":"advanced/prompts/#data-analyst","title":"Data Analyst","text":"<pre><code>You are a data analyst with expertise in statistics, data visualization, and data manipulation.\n\nWhen analyzing data or answering queries:\n1. Consider statistical validity and assumptions\n2. Suggest appropriate visualization techniques\n3. Recommend efficient data processing approaches\n4. Highlight potential biases or limitations\n5. Provide code examples in Python using pandas, matplotlib, or similar libraries\n\nAlways emphasize data integrity and ethical considerations in your analysis.\n</code></pre>"},{"location":"advanced/prompts/#system-administrator","title":"System Administrator","text":"<pre><code>You are a system administrator experienced with Linux, Windows, and cloud environments.\n\nWhen providing assistance:\n1. Prioritize security best practices\n2. Consider compatibility across different systems\n3. Provide step-by-step instructions with explanations\n4. Suggest monitoring and maintenance practices\n5. Mention potential pitfalls or issues to watch for\n\nFor shell commands, explain what each command does and any flags or options used.\n</code></pre>"},{"location":"advanced/prompts/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about chat history management</li> <li>Explore function calling capabilities</li> <li>Check out debugging and logs</li> </ul>"},{"location":"dev/add-provider/","title":"Add Provider","text":"<p>TODO: Add content for add-provider.</p>"},{"location":"dev/architecture/","title":"Architecture","text":"<p>TODO: Add content for architecture.</p>"},{"location":"dev/debug-cli/","title":"Debug Cli","text":"<p>TODO: Add content for debug-cli.</p>"},{"location":"providers/anthropic/","title":"Anthropic (Claude)","text":"<p>Anthropic's Claude models via the official Anthropic API.</p>"},{"location":"providers/anthropic/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=anthropic\nAPI_KEY=sk-ant-api03-your-key-here\nMODEL=claude-3-5-sonnet-20241022\nTEMPERATURE=0.3\nMAX_TOKENS=1024\n</code></pre>"},{"location":"providers/anthropic/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> Anthropic API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>TOP_K</code> Top-k sampling - <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code>"},{"location":"providers/anthropic/#aws-bedrock-integration","title":"AWS Bedrock Integration","text":"<pre><code>PROVIDER=anthropic_bedrock\nAWS_ACCESS_KEY_ID=your-access-key\nAWS_SECRET_ACCESS_KEY=your-secret-key\nAWS_REGION=us-east-1\nMODEL=anthropic.claude-3-5-sonnet-20241022-v2:0\n</code></pre>"},{"location":"providers/anthropic/#google-vertex-ai-integration","title":"Google Vertex AI Integration","text":"<pre><code>PROVIDER=anthropic_vertex\nPROJECT_ID=your-gcp-project-id\nCLOUD_ML_REGION=us-central1\nMODEL=claude-3-5-sonnet@20241022\n</code></pre>"},{"location":"providers/anthropic/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 System prompts</li> <li>\u2705 200K token context</li> </ul>"},{"location":"providers/anthropic/#important-notes","title":"Important Notes","text":"<ul> <li>API key starts with <code>sk-ant-</code></li> <li>Rate limits apply based on your plan</li> <li>System prompts are extracted automatically</li> <li>All models support vision capabilities</li> <li>Bedrock requires AWS credentials</li> <li>Vertex AI requires GCP authentication</li> </ul>"},{"location":"providers/bailian/","title":"Qwen","text":"<p>TODO: Add content for qwen.</p>"},{"location":"providers/cerebras/","title":"Cerebras","text":"<p>Cerebras Cloud SDK for fast inference on Cerebras hardware.</p>"},{"location":"providers/cerebras/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=cerebras\nAPI_KEY=your-cerebras-api-key\nMODEL=llama3.1-70b\nTEMPERATURE=0.7\nMAX_TOKENS=4096\n</code></pre>"},{"location":"providers/cerebras/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> Cerebras API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://api.cerebras.ai</code> <code>EXTRA_BODY</code> Additional request parameters <code>{}</code>"},{"location":"providers/cerebras/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 OpenAI-compatible API</li> <li>\u2705 Ultra-fast inference</li> <li>\u2705 Hardware acceleration</li> </ul>"},{"location":"providers/cerebras/#important-notes","title":"Important Notes","text":"<ul> <li>Uses Cerebras Cloud SDK with OpenAI compatibility</li> <li>Warm TCP connection disabled by default for better performance</li> <li>Optimized for speed with specialized hardware</li> <li>Uses max_completion_tokens instead of max_tokens internally</li> <li>Designed for high-throughput inference workloads</li> </ul>"},{"location":"providers/chatglm/","title":"Zhipu","text":"<p>TODO: Add content for zhipu.</p>"},{"location":"providers/cohere/","title":"Cohere","text":"<p>Cohere's Command models for text generation and analysis.</p>"},{"location":"providers/cohere/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=cohere\nAPI_KEY=your-cohere-api-key\nMODEL=command-r-plus-latest\nTEMPERATURE=0.7\n</code></pre>"},{"location":"providers/cohere/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> Cohere API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.7</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://api.cohere.com/v2</code> <code>ENVIRONMENT</code> Environment setting -"},{"location":"providers/cohere/#aws-bedrock-integration","title":"AWS Bedrock Integration","text":"<pre><code>PROVIDER=cohere_bedrock\nAWS_ACCESS_KEY_ID=your-access-key\nAWS_SECRET_ACCESS_KEY=your-secret-key\nAWS_REGION=us-east-1\nAWS_SESSION_TOKEN=your-session-token\nMODEL=cohere.command-r-plus-v1:0\n</code></pre>"},{"location":"providers/cohere/#aws-sagemaker-integration","title":"AWS SageMaker Integration","text":"<pre><code>PROVIDER=cohere_sagemaker\nAWS_ACCESS_KEY_ID=your-access-key\nAWS_SECRET_ACCESS_KEY=your-secret-key\nAWS_REGION=us-east-1\nAWS_SESSION_TOKEN=your-session-token\nMODEL=your-sagemaker-endpoint\n</code></pre>"},{"location":"providers/cohere/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 Tool planning</li> <li>\u2705 RAG capabilities</li> <li>\u2705 Multi-language support</li> </ul>"},{"location":"providers/cohere/#important-notes","title":"Important Notes","text":"<ul> <li>Supports OpenAI-compatible tool schemas</li> <li>Tool planning shows reasoning before tool calls</li> <li>Supports document format for tool responses</li> <li>RAG features with citation support</li> <li>Multiple deployment options (API, Bedrock, SageMaker)</li> </ul>"},{"location":"providers/deepseek/","title":"DeepSeek","text":"<p>DeepSeek's AI models via OpenAI-compatible API.</p>"},{"location":"providers/deepseek/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=deepseek\nAPI_KEY=sk-your-deepseek-api-key\nMODEL=deepseek-chat\nTEMPERATURE=0.3\nMAX_TOKENS=1024\n</code></pre>"},{"location":"providers/deepseek/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> DeepSeek API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://api.deepseek.com/v1</code> <code>EXTRA_BODY</code> Additional request parameters <code>{}</code>"},{"location":"providers/deepseek/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 OpenAI-compatible API</li> <li>\u2705 Code generation</li> <li>\u2705 Mathematical reasoning</li> </ul>"},{"location":"providers/deepseek/#important-notes","title":"Important Notes","text":"<ul> <li>Uses OpenAI-compatible client implementation</li> <li>Supports standard OpenAI API features</li> <li>Designed for coding and reasoning tasks</li> <li>Cost-effective alternative to other providers</li> </ul>"},{"location":"providers/doubao/","title":"Doubao (ByteDance)","text":"<p>ByteDance's Doubao models via VolcEngine ARK runtime.</p>"},{"location":"providers/doubao/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=doubao\nAPI_KEY=your-doubao-api-key\nMODEL=doubao-pro-32k\nTEMPERATURE=0.7\nMAX_TOKENS=4096\n</code></pre>"},{"location":"providers/doubao/#alternative-authentication","title":"Alternative Authentication","text":"<pre><code>PROVIDER=doubao\nAK=your-access-key\nSK=your-secret-key\nREGION=cn-beijing\nMODEL=doubao-pro-32k\nTEMPERATURE=0.7\nMAX_TOKENS=4096\n</code></pre>"},{"location":"providers/doubao/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> Doubao API key - <code>AK</code> Access key (alternative auth) - <code>SK</code> Secret key (alternative auth) - <code>REGION</code> Service region - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.7</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>4096</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://ark.cn-beijing.volces.com/api/v3</code> <code>EXTRA_BODY</code> Additional request parameters <code>{}</code>"},{"location":"providers/doubao/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 OpenAI-compatible API</li> <li>\u2705 Chinese language optimization</li> <li>\u2705 Multiple authentication methods</li> </ul>"},{"location":"providers/doubao/#important-notes","title":"Important Notes","text":"<ul> <li>Uses VolcEngine ARK runtime</li> <li>Based on OpenAI-compatible implementation</li> <li>Supports both API key and AK/SK authentication</li> <li>Optimized for Chinese language tasks</li> <li>Regional service deployment options</li> </ul>"},{"location":"providers/gemini/","title":"Gemini","text":"<p>TODO: Add content for gemini.</p>"},{"location":"providers/groq/","title":"Groq","text":"<p>Groq's fast inference platform for various open-source models.</p>"},{"location":"providers/groq/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=groq\nAPI_KEY=gsk_your-groq-api-key\nMODEL=llama-3.1-70b-versatile\nTEMPERATURE=0.3\nMAX_TOKENS=1024\n</code></pre>"},{"location":"providers/groq/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> Groq API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://api.groq.com/openai/v1</code> <code>REASONING_EFFORT</code> Reasoning effort level - <code>EXTRA_BODY</code> Additional request parameters <code>{}</code>"},{"location":"providers/groq/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 OpenAI-compatible API</li> <li>\u2705 Ultra-fast inference</li> <li>\u2705 Reasoning capabilities (qwen3 models)</li> </ul>"},{"location":"providers/groq/#important-notes","title":"Important Notes","text":"<ul> <li>Uses OpenAI-compatible client implementation</li> <li>N parameter is automatically set to 1 (Groq limitation)</li> <li>Reasoning effort only supported for qwen3 models</li> <li>Valid reasoning_effort values: <code>null</code>, <code>default</code></li> <li>Optimized for speed with high tokens per second</li> <li>Multiple open-source models available</li> </ul>"},{"location":"providers/huggingface/","title":"HuggingFace","text":"<pre><code># HuggingFace\n</code></pre> <p>HuggingFace Inference API for accessing various models.</p>"},{"location":"providers/huggingface/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=huggingface\nAPI_KEY=hf_your-huggingface-token\nMODEL=meta-llama/Llama-3.1-70B-Instruct\nTEMPERATURE=0.7\nHF_PROVIDER=auto\n</code></pre>"},{"location":"providers/huggingface/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> HuggingFace API token (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint - <code>HF_PROVIDER</code> HuggingFace provider <code>auto</code> <code>BILL_TO</code> Billing configuration - <code>EXTRA_HEADERS</code> Additional HTTP headers <code>{}</code>"},{"location":"providers/huggingface/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 Multiple model access</li> <li>\u2705 Serverless inference</li> <li>\u2705 Dedicated endpoints</li> <li>\u2705 Custom billing options</li> </ul>"},{"location":"providers/huggingface/#important-notes","title":"Important Notes","text":"<ul> <li>Based on ChatGLM provider implementation</li> <li>Uses HuggingFace InferenceClient</li> <li>Supports both serverless and dedicated endpoints</li> <li>Provider can be set to specific inference backends</li> <li>Billing can be configured for enterprise usage</li> <li>Wide variety of open-source models available</li> </ul>"},{"location":"providers/minimax/","title":"Minimax","text":"<p>TODO: Add content for minimax.</p>"},{"location":"providers/mistral/","title":"Mistral","text":"<p>Mistral AI's foundation models for general-purpose and specialized tasks.</p>"},{"location":"providers/mistral/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=mistral\nAPI_KEY=your-mistral-api-key\nMODEL=mistral-large-latest\nTEMPERATURE=0.7\nMAX_TOKENS=4096\n</code></pre>"},{"location":"providers/mistral/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> Mistral API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.7</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>4096</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://api.mistral.ai/v1</code> <code>SERVER</code> Server configuration - <code>EXTRA_HEADERS</code> Additional HTTP headers <code>{}</code>"},{"location":"providers/mistral/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 Parallel tool calls (disabled by default)</li> <li>\u2705 Document processing</li> <li>\u2705 Image analysis</li> </ul>"},{"location":"providers/mistral/#important-notes","title":"Important Notes","text":"<ul> <li>Timeout is specified in seconds but converted to milliseconds internally</li> <li>Parallel tool calls are disabled for better compatibility</li> <li>Supports document and image content types</li> <li>Reference handling for RAG applications</li> <li>Custom server configurations supported</li> </ul>"},{"location":"providers/modelscope/","title":"Dashscope","text":"<p>TODO: Add content for dashscope.</p>"},{"location":"providers/moonshot/","title":"Moonshot (\u6708\u4e4b\u6697\u9762)","text":"<p>Moonshot's AI models via OpenAI-compatible API.</p>"},{"location":"providers/moonshot/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=moonshot\nAPI_KEY=sk-your-moonshot-api-key\nMODEL=moonshot-v1-8k\nTEMPERATURE=0.3\nMAX_TOKENS=1024\n</code></pre>"},{"location":"providers/moonshot/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> Moonshot API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://api.moonshot.cn/v1</code> <code>EXTRA_BODY</code> Additional request parameters <code>{}</code>"},{"location":"providers/moonshot/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 OpenAI-compatible API</li> <li>\u2705 Chinese language optimization</li> <li>\u2705 Long context windows</li> </ul>"},{"location":"providers/moonshot/#important-notes","title":"Important Notes","text":"<ul> <li>Uses OpenAI-compatible client implementation</li> <li>Developed by Chinese AI company \u6708\u4e4b\u6697\u9762 (Dark Side of the Moon)</li> <li>Optimized for Chinese language understanding</li> <li>Supports standard OpenAI API features</li> <li>Good performance on Chinese NLP tasks</li> </ul>"},{"location":"providers/nvidia/","title":"NVIDIA NIM","text":"<p>NVIDIA's NIM (NVIDIA Inference Microservice) platform.</p>"},{"location":"providers/nvidia/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=nvidia\nAPI_KEY=nvapi-your-nvidia-api-key\nMODEL=meta/llama-3.1-70b-instruct\nTEMPERATURE=0.3\nMAX_TOKENS=1024\n</code></pre>"},{"location":"providers/nvidia/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> NVIDIA API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://integrate.api.nvidia.com/v1</code> <code>EXTRA_BODY</code> Additional request parameters <code>{}</code>"},{"location":"providers/nvidia/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 OpenAI-compatible API</li> <li>\u2705 GPU-optimized inference</li> <li>\u2705 Enterprise deployment</li> </ul>"},{"location":"providers/nvidia/#important-notes","title":"Important Notes","text":"<ul> <li>Uses OpenAI-compatible client implementation</li> <li>Automatically adds chat_template_kwargs to extra_body</li> <li>Supports thinking mode for compatible models (Qwen3/Granite)</li> <li>NVIDIA API accepts redundant parameters gracefully</li> <li>Optimized for GPU inference workloads</li> <li>Enterprise-grade security and compliance</li> </ul>"},{"location":"providers/ollama/","title":"Ollama","text":"<p>Local LLM hosting platform for running models on your own hardware.</p>"},{"location":"providers/ollama/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=ollama\nMODEL=llama3.2:3b\nBASE_URL=http://localhost:11434\nTEMPERATURE=0.7\nENABLE_FUNCTIONS=true\n</code></pre>"},{"location":"providers/ollama/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>MODEL</code> Model name (required) - <code>BASE_URL</code> Ollama server URL <code>http://localhost:11434</code> <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.7</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>THINK</code> Enable reasoning mode <code>false</code>"},{"location":"providers/ollama/#advanced-options","title":"Advanced Options","text":"Parameter Description Default <code>SEED</code> Random seed - <code>NUM_PREDICT</code> Max tokens to generate - <code>NUM_CTX</code> Context window size - <code>NUM_BATCH</code> Batch size - <code>NUM_GPU</code> GPU layers - <code>NUM_THREAD</code> CPU threads -"},{"location":"providers/ollama/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 Local execution</li> <li>\u2705 No API costs</li> <li>\u2705 Privacy preservation</li> <li>\u2705 Custom models</li> </ul>"},{"location":"providers/ollama/#installation","title":"Installation","text":"<p>Install Ollama first:</p> <pre><code># macOS\nbrew install ollama\n\n# Linux\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Windows\n# Download from https://ollama.ai/download\n</code></pre> <p>Pull models:</p> <pre><code>ollama pull llama3.2:3b\nollama pull qwen3:7b\n</code></pre>"},{"location":"providers/ollama/#important-notes","title":"Important Notes","text":"<ul> <li>Requires Ollama server running locally</li> <li>Models must be pulled before use</li> <li>Performance depends on hardware specs</li> <li>Supports OpenAI-compatible tool calling</li> <li>Reasoning mode available with <code>THINK=true</code> in config</li> <li>GPU acceleration recommended for larger models</li> </ul>"},{"location":"providers/openai/","title":"Openai","text":"<p>TODO: Add content for openai.</p>"},{"location":"providers/openrouter/","title":"OpenRouter","text":"<p>OpenRouter's unified API for accessing multiple LLM providers.</p>"},{"location":"providers/openrouter/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=openrouter\nAPI_KEY=sk-or-your-openrouter-api-key\nMODEL=anthropic/claude-3.5-sonnet\nTEMPERATURE=0.3\nMAX_TOKENS=1024\n</code></pre>"},{"location":"providers/openrouter/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> OpenRouter API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://openrouter.ai/api/v1</code> <code>EXTRA_BODY</code> Additional request parameters <code>{}</code>"},{"location":"providers/openrouter/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 OpenAI-compatible API</li> <li>\u2705 Multiple providers</li> <li>\u2705 Unified pricing</li> </ul>"},{"location":"providers/openrouter/#important-notes","title":"Important Notes","text":"<ul> <li>Uses OpenAI-compatible client implementation</li> <li>Access to models from multiple providers</li> <li>Single API key for all providers</li> <li>Transparent pricing and usage tracking</li> <li>Model routing and fallback options</li> <li>Credits-based billing system</li> </ul>"},{"location":"providers/overview/","title":"Provider Overview &amp; Feature Matrix","text":"<p>YAICLI supports a wide range of LLM providers, giving you flexibility to choose the model that best fits your needs. This page provides an overview of all supported providers and their capabilities.</p>"},{"location":"providers/overview/#supported-providers","title":"Supported Providers","text":"<p>YAICLI currently integrates with the following LLM providers:</p>"},{"location":"providers/overview/#major-cloud-providers","title":"Major Cloud Providers","text":"Provider Description Default Base URL API Documentation OpenAI GPT models and advanced AI <code>https://api.openai.com/v1</code> Docs Anthropic (Claude) Advanced reasoning and safety <code>https://api.anthropic.com</code> Docs Google Gemini Multimodal AI models <code>https://generativelanguage.googleapis.com/v1beta/openai</code> Docs Cohere Enterprise-focused models <code>https://api.cohere.com/v2</code> Docs Mistral European AI models <code>https://api.mistral.ai/v1</code> Docs"},{"location":"providers/overview/#high-performance-inference","title":"High-Performance Inference","text":"Provider Description Default Base URL API Documentation Groq Ultra-fast inference platform <code>https://api.groq.com/openai/v1</code> Docs Cerebras Hardware-accelerated inference <code>https://api.cerebras.ai</code> Docs NVIDIA NIM GPU-optimized models <code>https://integrate.api.nvidia.com/v1</code> Docs SambaNova High-performance AI platform <code>https://api.sambanova.ai/v1</code> Docs"},{"location":"providers/overview/#multi-provider-aggregation","title":"Multi-Provider &amp; Aggregation","text":"Provider Description Default Base URL API Documentation OpenRouter Unified API for multiple providers <code>https://openrouter.ai/api/v1</code> Docs Together Open-source model platform <code>https://api.together.xyz/v1</code> Docs HuggingFace Open-source model hub - Docs"},{"location":"providers/overview/#cloud-platform-integration","title":"Cloud Platform Integration","text":"Provider Description Default Base URL API Documentation Vertex AI Google Cloud AI platform - Docs"},{"location":"providers/overview/#specialized-regional","title":"Specialized &amp; Regional","text":"Provider Description Default Base URL API Documentation DeepSeek Code and reasoning specialist <code>https://api.deepseek.com/v1</code> Docs XAI (Grok) Real-time information models <code>https://api.xai.com/v1</code> Docs Yi (01.AI) Multilingual AI models <code>https://api.lingyiwanwu.com/v1</code> Docs Doubao ByteDance AI models <code>https://ark.cn-beijing.volces.com/api/v3</code> Docs ChatGLM Zhipu AI models <code>https://open.bigmodel.cn/api/paas/v4/</code> Docs Moonshot Chinese AI models <code>https://api.moonshot.cn/v1</code> Docs Minimax Chinese multimodal models <code>https://api.minimaxi.com/v1</code> Docs ModelScope Alibaba's model platform <code>https://api-inference.modelscope.cn/v1/</code> Docs Bailian Alibaba Cloud AI <code>https://dashscope.aliyuncs.com/compatible-mode/v1</code> Docs"},{"location":"providers/overview/#local-self-hosted","title":"Local &amp; Self-Hosted","text":"Provider Description Default Base URL API Documentation Ollama Local model hosting <code>http://localhost:11434</code> Docs"},{"location":"providers/overview/#feature-comparison","title":"Feature Comparison","text":""},{"location":"providers/overview/#core-features","title":"Core Features","text":"Provider Streaming Function Calling MCP Support OpenAI \u2705 \u2705 \u2705 Anthropic (Claude) \u2705 \u2705 \u2705 Google Gemini \u2705 \u2705 \u2705 Cohere \u2705 \u2705 \u2705 Mistral \u2705 \u2705 \u2705 Groq \u2705 \u2705 \u2705 Cerebras \u2705 \u2705 \u2705 NVIDIA NIM \u2705 \u2705 \u2705 SambaNova \u2705 \u2705 \u2705 OpenRouter \u2705 \u2705 \u2705 Together \u2705 \u2705 \u2705 HuggingFace \u2705 \u2705 \u2705 Vertex AI \u2705 \u2705 \u2705 DeepSeek \u2705 \u2705 \u2705 XAI (Grok) \u2705 \u2705 \u2705 Yi (01.AI) \u2705 \u2705 \u2705 Doubao \u2705 \u2705 \u2705 ChatGLM \u2705 \u2705 \u2705 Ollama \u2705 \u2705 \u2705"},{"location":"providers/overview/#performance-characteristics","title":"Performance Characteristics","text":"Provider Speed Cost Context Special Features Groq \ud83d\udd25\ud83d\udd25\ud83d\udd25 \ud83d\udcb0\ud83d\udcb0 Standard Ultra-fast inference Cerebras \ud83d\udd25\ud83d\udd25\ud83d\udd25 \ud83d\udcb0\ud83d\udcb0\ud83d\udcb0 Standard Hardware acceleration OpenAI \ud83d\udd25\ud83d\udd25 \ud83d\udcb0\ud83d\udcb0\ud83d\udcb0 Large Industry standard Anthropic \ud83d\udd25\ud83d\udd25 \ud83d\udcb0\ud83d\udcb0\ud83d\udcb0 Very Large Safety-focused Ollama \ud83d\udd25 Free Varies Local, private DeepSeek \ud83d\udd25\ud83d\udd25 \ud83d\udcb0 Large Code specialist Together \ud83d\udd25\ud83d\udd25 \ud83d\udcb0 Standard Open source focus <p>Legend: \ud83d\udd25 = Speed level, \ud83d\udcb0 = Cost level</p>"},{"location":"providers/overview/#optional-dependencies","title":"Optional Dependencies","text":"<p>Some providers require additional dependencies that can be installed using pip:</p> <pre><code># Install all dependencies\npip install 'yaicli[all]'\n\n# Install specific provider dependencies\npip install 'yaicli[ollama,cohere,doubao,huggingface,gemini,mistral,anthropic]'\n</code></pre>"},{"location":"providers/overview/#provider-specific-documentation","title":"Provider-Specific Documentation","text":"<p>For detailed configuration options for each provider, refer to the provider-specific pages:</p>"},{"location":"providers/overview/#major-cloud-providers_1","title":"Major Cloud Providers","text":"<ul> <li>OpenAI</li> <li>Anthropic (Claude)</li> <li>Google Gemini</li> <li>DeepSeek</li> <li>Cohere</li> <li>Mistral</li> <li>ChatGLM</li> <li>Moonshot</li> <li>XAI (Grok)</li> <li>Yi (01.AI)</li> <li>Doubao</li> <li>Minimax</li> </ul>"},{"location":"providers/overview/#high-performance-inference_1","title":"High-Performance Inference","text":"<ul> <li>Groq</li> <li>Cerebras</li> <li>NVIDIA NIM</li> <li>SambaNova</li> </ul>"},{"location":"providers/overview/#multi-provider-aggregation_1","title":"Multi-Provider &amp; Aggregation","text":"<ul> <li>OpenRouter</li> <li>Together</li> <li>HuggingFace</li> <li>ModelScope</li> <li>Bailian</li> </ul>"},{"location":"providers/overview/#cloud-platform-integration_1","title":"Cloud Platform Integration","text":"<ul> <li>Vertex AI</li> </ul>"},{"location":"providers/overview/#local-self-hosted_1","title":"Local &amp; Self-Hosted","text":"<ul> <li>Ollama</li> </ul>"},{"location":"providers/overview/#using-custom-openai-compatible-endpoints","title":"Using Custom OpenAI-Compatible Endpoints","text":"<p>Many providers offer OpenAI-compatible endpoints. To use them:</p> <pre><code>PROVIDER=openai\nBASE_URL=https://your-custom-endpoint.com/v1\nAPI_KEY=your-api-key\nMODEL=model-name\n</code></pre>"},{"location":"providers/overview/#switching-providers-via-environment-variables","title":"Switching Providers via Environment Variables","text":"<p>You can temporarily switch providers using environment variables:</p> <pre><code>export YAI_PROVIDER=gemini\nexport YAI_API_KEY=AI...\nexport YAI_MODEL=gemini-2.5-flash\nai \"What is quantum computing?\"\n</code></pre> <p>This overrides your config file settings for the current session.</p>"},{"location":"providers/sambanova/","title":"SambaNova","text":"<p>SambaNova's AI platform for high-performance inference.</p>"},{"location":"providers/sambanova/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=sambanova\nAPI_KEY=your-sambanova-api-key\nMODEL=Meta-Llama-3.1-405B-Instruct\nTEMPERATURE=0.3\nMAX_TOKENS=1024\n</code></pre>"},{"location":"providers/sambanova/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> SambaNova API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://api.sambanova.ai/v1</code> <code>EXTRA_BODY</code> Additional request parameters <code>{}</code>"},{"location":"providers/sambanova/#function-call-support","title":"Function Call Support","text":"<p>Only specific models support function calling:</p> <ul> <li><code>Meta-Llama-3.1-8B-Instruct</code></li> <li><code>Meta-Llama-3.1-405B-Instruct</code></li> <li><code>Meta-Llama-3.3-70B-Instruct</code></li> <li><code>Llama-4-Scout-17B-16E-Instruct</code></li> <li><code>DeepSeek-V3-0324</code></li> </ul>"},{"location":"providers/sambanova/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling (select models)</li> <li>\u2705 MCP support</li> <li>\u2705 OpenAI-compatible API</li> <li>\u2705 High-performance inference</li> <li>\u2705 Large model support</li> </ul>"},{"location":"providers/sambanova/#important-notes","title":"Important Notes","text":"<ul> <li>Uses OpenAI-compatible client implementation</li> <li>Temperature automatically clamped to 0.0-1.0 range</li> <li>Function calling only supported on specific models</li> <li>Warning displayed if function calling used with unsupported model</li> <li>Optimized for large-scale model inference</li> </ul>"},{"location":"providers/together/","title":"Together","text":"<p>Together AI's platform for accessing various open-source models.</p>"},{"location":"providers/together/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=together\nAPI_KEY=your-together-api-key\nMODEL=meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\nTEMPERATURE=0.3\nMAX_TOKENS=1024\n</code></pre>"},{"location":"providers/together/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> Together API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://api.together.xyz/v1</code> <code>EXTRA_BODY</code> Additional request parameters <code>{}</code>"},{"location":"providers/together/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 OpenAI-compatible API</li> <li>\u2705 Multiple open-source models</li> <li>\u2705 Cost-effective inference</li> </ul>"},{"location":"providers/together/#important-notes","title":"Important Notes","text":"<ul> <li>Uses OpenAI-compatible client implementation</li> <li>Access to various open-source models</li> <li>Competitive pricing for open-source models</li> <li>Supports latest Llama, Qwen, and other models</li> <li>Good alternative for open-source model access</li> </ul>"},{"location":"providers/vertexai/","title":"Vertex AI","text":"<p>Google Cloud Vertex AI platform for accessing Gemini and other models.</p>"},{"location":"providers/vertexai/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=vertexai\nPROJECT=your-gcp-project-id\nLOCATION=us-central1\nMODEL=gemini-1.5-pro\nTEMPERATURE=0.3\n</code></pre>"},{"location":"providers/vertexai/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>PROJECT</code> GCP project ID (required) - <code>LOCATION</code> GCP region (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>TOP_K</code> Top-k sampling - <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code>"},{"location":"providers/vertexai/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 Vision capabilities</li> <li>\u2705 Enterprise features</li> <li>\u2705 Multi-modal support</li> </ul>"},{"location":"providers/vertexai/#authentication","title":"Authentication","text":"<p>Vertex AI requires Google Cloud authentication. Set up authentication using one of these methods:</p> <pre><code># Application Default Credentials\ngcloud auth application-default login\n\n# Service Account Key\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n\n# Workload Identity (for GKE)\n# Configured automatically in GKE environment\n</code></pre>"},{"location":"providers/vertexai/#important-notes","title":"Important Notes","text":"<ul> <li>Based on Gemini provider implementation</li> <li>Requires GCP project and location configuration</li> <li>Automatic authentication via Google Cloud SDK</li> <li>Enterprise-grade security and compliance</li> <li>Regional model availability may vary</li> <li>Billing through Google Cloud Console</li> </ul>"},{"location":"providers/xai/","title":"XAI (Grok)","text":"<p>X AI's Grok models via OpenAI-compatible API.</p>"},{"location":"providers/xai/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=xai\nAPI_KEY=xai-your-api-key\nMODEL=grok-2-latest\nTEMPERATURE=0.3\nMAX_TOKENS=1024\n</code></pre>"},{"location":"providers/xai/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> XAI API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://api.xai.com/v1</code> <code>EXTRA_BODY</code> Additional request parameters <code>{}</code>"},{"location":"providers/xai/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 OpenAI-compatible API</li> <li>\u2705 Real-time information access</li> <li>\u2705 Multimodal capabilities</li> </ul>"},{"location":"providers/xai/#important-notes","title":"Important Notes","text":"<ul> <li>Uses OpenAI-compatible client implementation</li> <li>Built by X (formerly Twitter) AI team</li> <li>Designed for real-time information and analysis</li> <li>Supports standard OpenAI API features</li> <li>Integration with X platform data when available</li> </ul>"},{"location":"providers/yi/","title":"Yi (01.AI)","text":"<p>01.AI's Yi models via OpenAI-compatible API.</p>"},{"location":"providers/yi/#configuration","title":"Configuration","text":"<pre><code>PROVIDER=yi\nAPI_KEY=your-yi-api-key\nMODEL=yi-large\nTEMPERATURE=0.3\nMAX_TOKENS=1024\n</code></pre>"},{"location":"providers/yi/#key-parameters","title":"Key Parameters","text":"Parameter Description Default <code>API_KEY</code> Yi API key (required) - <code>MODEL</code> Model to use - <code>TEMPERATURE</code> Randomness (0.0-1.0) <code>0.3</code> <code>TOP_P</code> Nucleus sampling <code>1.0</code> <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>TIMEOUT</code> Request timeout (seconds) <code>60</code> <code>BASE_URL</code> Custom API endpoint <code>https://api.lingyiwanwu.com/v1</code> <code>EXTRA_BODY</code> Additional request parameters <code>{}</code>"},{"location":"providers/yi/#features","title":"Features","text":"<ul> <li>\u2705 Streaming responses</li> <li>\u2705 Function calling</li> <li>\u2705 MCP support</li> <li>\u2705 OpenAI-compatible API</li> <li>\u2705 Multilingual support</li> <li>\u2705 Long context windows</li> </ul>"},{"location":"providers/yi/#important-notes","title":"Important Notes","text":"<ul> <li>Uses OpenAI-compatible client implementation</li> <li>Developed by 01.AI (Kai-Fu Lee's company)</li> <li>Strong performance in Chinese and English</li> <li>Competitive with leading international models</li> <li>Cost-effective alternative for Asian markets</li> </ul>"},{"location":"usage/advanced/","title":"Advanced Features","text":"<p>YAICLI includes several advanced features to enhance your interaction with AI models.</p>"},{"location":"usage/advanced/#chat-persistent-sessions","title":"Chat Persistent Sessions","text":"<p>YAICLI allows you to save and manage chat sessions:</p> <pre><code># Start a temporary chat session\n$ ai --chat\n\n# Start a persistent chat session with a title\n$ ai --chat \"Python programming help\"\n\n# List saved chat sessions\n$ ai --list-chats\n</code></pre>"},{"location":"usage/advanced/#managing-chat-sessions","title":"Managing Chat Sessions","text":"<p>In a chat session, you can use these commands: - <code>/save &lt;title&gt;</code> - Save the current session with a title - <code>/list</code> - List all saved sessions - <code>/load &lt;index&gt;</code> - Load a previously saved session - <code>/del &lt;index&gt;</code> - Delete a saved session</p>"},{"location":"usage/advanced/#persisting-chat-sessions","title":"Persisting Chat Sessions","text":"<p>When you start a chat with a title, the session is automatically saved to disk:</p> <pre><code>$ ai --chat \"check disk usage\"\nChat title: check disk usage\n</code></pre> <p>The session will be persisted to <code>CHAT_HISTORY_DIR</code> by default.</p>"},{"location":"usage/advanced/#role-management","title":"Role Management","text":"<p>YAICLI supports custom roles to guide AI behavior:</p> <pre><code># Create a new role\nai --create-role \"Python Expert\"\n\n# List all roles\nai --list-roles\n\n# Show a role\nai --show-role \"Python Expert\"\n\n# Delete a role\nai --delete-role \"Python Expert\"\n\n# Use a specific role\nai --role \"Python Expert\" \"How do I use decorators?\"\n</code></pre>"},{"location":"usage/advanced/#function-calling","title":"Function Calling","text":"<p>YAICLI supports function calling, enabling AI models to call functions to accomplish tasks:</p> <pre><code># Install default functions\nai --install-functions\n\n# List available functions\nai --list-functions\n\n# Enable function calling in queries\nai 'check the current dir total size' --enable-functions\n</code></pre> <p>You can also define custom functions by adding them to <code>~/.config/yaicli/functions/</code>.</p>"},{"location":"usage/advanced/#mcp-machine-comprehension-protocol","title":"MCP (Machine Comprehension Protocol)","text":"<p>MCP allows AI models to use external tools:</p> <pre><code># Enable MCP in queries\nai 'What is the latest exchange rate between BTC and USD?' --enable-mcp\n</code></pre> <p>Configure MCP tools in <code>~/.config/yaicli/mcp.json</code>.</p>"},{"location":"usage/advanced/#custom-configuration","title":"Custom Configuration","text":""},{"location":"usage/advanced/#llm-provider-configuration","title":"LLM Provider Configuration","text":"<p>YAICLI works with many LLM providers. Configure your preferred provider in the config file:</p> <pre><code>[core]\nPROVIDER=openai  # or cohere, gemini, ollama, etc.\nBASE_URL=        # leave empty for default provider URL\nAPI_KEY=your_api_key_here\nMODEL=gpt-4o\n</code></pre>"},{"location":"usage/advanced/#syntax-highlighting-themes","title":"Syntax Highlighting Themes","text":"<p>Change the code highlighting theme:</p> <pre><code>CODE_THEME = monokai\n</code></pre> <p>Browse available themes at: https://pygments.org/styles/</p> <p></p>"},{"location":"usage/advanced/#extra-headers-and-body-parameters","title":"Extra Headers and Body Parameters","text":"<p>Add custom parameters to API requests:</p> <pre><code>EXTRA_HEADERS={\"X-Extra-Header\": \"value\"}\nEXTRA_BODY={\"extra_key\": \"extra_value\"}\n</code></pre> <p>Example for Qwen3's thinking behavior:</p> <pre><code>EXTRA_BODY={\"enable_thinking\": false}\n# Or limit thinking tokens:\nEXTRA_BODY={\"thinking_budget\": 4096}\n</code></pre>"},{"location":"usage/basic/","title":"Basic Usage","text":"<p>YAICLI offers multiple ways to interact with AI models directly from your command line.</p>"},{"location":"usage/basic/#quick-start","title":"Quick Start","text":"<p>Get a quick answer.</p> <pre><code>ai \"What is the capital of France?\"\n</code></pre> <p>Start an interactive chat session <pre><code>ai --chat\n</code></pre></p> <p>Generate and execute shell commands <pre><code>ai --shell \"Create a backup of my Documents folder\"\n</code></pre></p> <p>Generate code snippets, default in Python <pre><code>ai --code \"Write a Python function to sort a list\"\n</code></pre></p> <p>Read stdin from pipe <pre><code>cat app.py | ai \"Explain what this code does\"\n</code></pre></p> <p>Debug with verbose mode <pre><code>ai --verbose \"Explain quantum computing\"\n</code></pre></p>"},{"location":"usage/basic/#command-line-reference","title":"Command Line Reference","text":"<pre><code> Usage: ai [OPTIONS] [PROMPT]\n\n YAICLI: Your AI assistant in the command line.\n\n Call with a PROMPT to get a direct answer, use --shell to execute as command, or use --chat for an interactive session.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502   prompt      [PROMPT]  The prompt to send to the LLM. Reads from stdin if available. [default: None]                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --install-completion            Install completion for the current shell.                                              \u2502\n\u2502 --show-completion               Show completion for the current shell, to copy it or customize the installation.       \u2502\n\u2502 --help                -h        Show this message and exit.                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 LLM Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --model        -M                 TEXT                       Specify the model to use.                                 \u2502\n\u2502 --temperature  -T                 FLOAT RANGE [0.0&lt;=x&lt;=2.0]  Specify the temperature to use. [default: 0.3]            \u2502\n\u2502 --top-p        -P                 FLOAT RANGE [0.0&lt;=x&lt;=1.0]  Specify the top-p to use. [default: 1.0]                  \u2502\n\u2502 --max-tokens                      INTEGER RANGE [x&gt;=1]       Specify the max tokens to use. [default: 2048]            \u2502\n\u2502 --stream           --no-stream                               Specify whether to stream the response. (default: stream) \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Role Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --role         -r      TEXT  Specify the assistant role to use. [default: DEFAULT]                                     \u2502\n\u2502 --create-role          TEXT  Create a new role with the specified name.                                                \u2502\n\u2502 --delete-role          TEXT  Delete a role with the specified name.                                                    \u2502\n\u2502 --list-roles                 List all available roles.                                                                 \u2502\n\u2502 --show-role            TEXT  Show the role with the specified name.                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Chat Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --chat        -c        Start in interactive chat mode.                                                                \u2502\n\u2502 --list-chats            List saved chat sessions.                                                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Shell Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --shell  -s        Generate and optionally execute a shell command (non-interactive).                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Code Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --code          Generate code in plaintext (non-interactive).                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Other Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --verbose         -V                                                        Show verbose output (e.g., loaded config). \u2502\n\u2502 --template                                                                  Show the default config file template and  \u2502\n\u2502                                                                             exit.                                      \u2502\n\u2502 --show-reasoning      --hide-reasoning                                      Show reasoning content from the LLM.       \u2502\n\u2502                                                                             (default: show)                            \u2502\n\u2502 --justify         -j                      [default|left|center|right|full]  Specify the justify to use.                \u2502\n\u2502                                                                             [default: default]                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Function Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --install-functions                                   Install default functions.                                       \u2502\n\u2502 --list-functions                                      List all available functions.                                    \u2502\n\u2502 --enable-functions        --disable-functions         Enable/disable function calling in API requests (default:        \u2502\n\u2502                                                       disabled)                                                        \u2502\n\u2502 --show-function-output    --hide-function-output      Show the output of functions (default: show)                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 MCP Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --enable-mcp         --disable-mcp          Enable/disable MCP in API requests (default: disabled)                     \u2502\n\u2502                                             [default: disable-mcp]                                                     \u2502\n\u2502 --show-mcp-output    --hide-mcp-output      Show the output of MCP (default: show)                                     \u2502\n\u2502 --list-mcp                                  List all available mcp.                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"usage/cli/","title":"CLI Shortcuts &amp; Hotkeys","text":"<p>YAICLI offers a rich set of keyboard shortcuts and interactive features to enhance your command-line experience. This page covers all available shortcuts and interactive functionalities.</p>"},{"location":"usage/cli/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":""},{"location":"usage/cli/#global-shortcuts","title":"Global Shortcuts","text":"<p>These shortcuts work in all interactive modes:</p> Shortcut Description <code>Ctrl+C</code> or <code>Ctrl+D</code> Exit the application <code>Tab</code> Toggle between Chat/Execute modes <code>Ctrl+R</code> Search command history <code>\u2191/\u2193</code> Navigate through command history <code>Ctrl+L</code> Clear the screen <code>Ctrl+A</code> Move cursor to beginning of line <code>Ctrl+E</code> Move cursor to end of line <code>Ctrl+U</code> Clear charactors before cursor <code>Ctrl+W</code> Delete word before cursor"},{"location":"usage/cli/#history-navigation","title":"History Navigation","text":"<p>When searching history with <code>Ctrl+R</code>:</p> Shortcut Description <code>Ctrl+R</code> Cycle through matching history items <code>Enter</code> Select current history item <code>Esc</code> Cancel history search"},{"location":"usage/cli/#interactive-commands","title":"Interactive Commands","text":"<p>YAICLI provides special commands that begin with a slash (<code>/</code>):</p> Command Description <code>/help</code> or <code>/?</code> Show help message <code>/clear</code> Clear the current conversation history <code>/his</code> Show command history <code>/list</code> List all saved chat sessions <code>/save &lt;title&gt;</code> Save the current chat session with a title <code>/load &lt;index&gt;</code> Load a saved chat session <code>/del &lt;index&gt;</code> Delete a saved chat session <code>/exit</code> Exit the application <code>/mode chat\\|exec</code> Switch between chat and execute modes"},{"location":"usage/cli/#auto-suggestion","title":"Auto-Suggestion","text":"<p>YAICLI offers auto-suggestions based on your command history. As you type, it shows light gray text suggesting completions from your history:</p> <ul> <li>Press <code>\u2192</code> (right arrow) to accept the full suggestion</li> <li>Continue typing to ignore the suggestion</li> <li>Press <code>Ctrl+\u2192</code> to accept the next word of the suggestion</li> </ul> <p>To disable auto-suggestions, set <code>AUTO_SUGGEST=false</code> in your configuration.</p>"},{"location":"usage/cli/#multi-line-input","title":"Multi-line Input","text":"<p>To enter multi-line input:</p> <ol> <li>Type your text normally</li> <li>Press <code>Alt+Enter</code> or <code>Esc+Enter</code> to add a new line</li> <li>Continue adding lines as needed</li> <li>Press <code>Enter</code> on an empty line to submit</li> </ol> <p>This is useful for: - Writing multi-line code examples - Composing complex prompts - Structuring lists or tables</p>"},{"location":"usage/cli/#command-editing","title":"Command Editing","text":"<p>When in Execute mode and reviewing a suggested command:</p> Option Description <code>e</code> Edit the command before execution <code>y</code> Execute the command as is <code>n</code> Cancel execution <p>When editing a command: - The full command is presented in an editable field - Make your changes and press <code>Enter</code> to execute - Press <code>Esc</code> to cancel the edit</p>"},{"location":"usage/cli/#visual-indicators","title":"Visual Indicators","text":"<p>YAICLI uses visual indicators to help you understand the current state:</p> Indicator Meaning <code>\ud83d\udcac &gt;</code> Chat mode prompt <code>\ud83d\ude80 &gt;</code> Execute mode prompt Cursor animation Processing your request Syntax highlighting Code in responses"},{"location":"usage/cli/#input-methods","title":"Input Methods","text":"<p>YAICLI accepts input through various methods:</p>"},{"location":"usage/cli/#direct-input","title":"Direct Input","text":"<pre><code>ai \"What is the capital of France?\"\n</code></pre>"},{"location":"usage/cli/#piped-input","title":"Piped Input","text":"<pre><code>echo \"What is the capital of France?\" | ai\n</code></pre>"},{"location":"usage/cli/#file-analysis","title":"File Analysis","text":"<pre><code>cat error.log | ai \"Explain these errors\"\n</code></pre>"},{"location":"usage/cli/#combined-input","title":"Combined Input","text":"<pre><code>cat data.csv | ai \"Convert this CSV to JSON\"\n</code></pre>"},{"location":"usage/cli/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about different interaction modes</li> <li>Explore configuration options</li> <li>Check out command references</li> </ul>"},{"location":"usage/commands/","title":"Command Overview","text":"<p>YAICLI provides a versatile command-line interface with multiple options and modes. This page covers all available commands and options.</p>"},{"location":"usage/commands/#basic-syntax","title":"Basic Syntax","text":"<pre><code>ai [OPTIONS] [PROMPT]\n</code></pre> <p>Where <code>PROMPT</code> is the text input for the AI assistant. If not provided, YAICLI will read from stdin if available.</p>"},{"location":"usage/commands/#command-line-reference","title":"Command Line Reference","text":"<p>Here's a comprehensive list of all available options:</p>"},{"location":"usage/commands/#core-options","title":"Core Options","text":"Option Short Description <code>--help</code> <code>-h</code> Show help message and exit <code>--verbose</code> <code>-V</code> Show verbose output (loaded config, API calls, etc.) <code>--template</code> Show the default config file template and exit"},{"location":"usage/commands/#mode-options","title":"Mode Options","text":"Option Short Description <code>--chat</code> <code>-c</code> Start in interactive chat mode <code>--shell</code> <code>-s</code> Generate and optionally execute shell commands <code>--code</code> Generate code in plaintext"},{"location":"usage/commands/#llm-options","title":"LLM Options","text":"Option Short Description Default <code>--model</code> <code>-M</code> Specify the model to use Configured default <code>--temperature</code> <code>-T</code> Set temperature (randomness) 0.5 <code>--top-p</code> <code>-P</code> Set top-p sampling 1.0 <code>--max-tokens</code> Set max tokens in response 1024 <code>--stream</code> / <code>--no-stream</code> Enable/disable streaming stream"},{"location":"usage/commands/#role-options","title":"Role Options","text":"Option Short Description <code>--role</code> <code>-r</code> Specify the assistant role to use <code>--create-role</code> Create a new role with the specified name <code>--delete-role</code> Delete a role with the specified name <code>--list-roles</code> List all available roles <code>--show-role</code> Show the role with the specified name"},{"location":"usage/commands/#chat-options","title":"Chat Options","text":"Option Description <code>--list-chats</code> List saved chat sessions"},{"location":"usage/commands/#display-options","title":"Display Options","text":"Option Short Description Default <code>--justify</code> <code>-j</code> Text alignment (default, left, center, right, full) default <code>--show-reasoning</code> / <code>--hide-reasoning</code> Show/hide reasoning content show"},{"location":"usage/commands/#function-options","title":"Function Options","text":"Option Description Default <code>--install-functions</code> Install default functions <code>--list-functions</code> List all available functions <code>--enable-functions</code> / <code>--disable-functions</code> Enable/disable function calling disabled <code>--show-function-output</code> / <code>--hide-function-output</code> Show/hide function output show"},{"location":"usage/commands/#mcp-options","title":"MCP Options","text":"Option Description Default <code>--enable-mcp</code> / <code>--disable-mcp</code> Enable/disable MCP in API requests disabled <code>--show-mcp-output</code> / <code>--hide-mcp-output</code> Show/hide MCP output show <code>--list-mcp</code> List all available MCP"},{"location":"usage/commands/#common-command-examples","title":"Common Command Examples","text":"<pre><code># Get a quick answer\nai \"What is the capital of France?\"\n\n# Start an interactive chat session\nai --chat\n\n# Save chat with a title\nai --chat \"Python programming tips\"\n\n# Generate and execute shell commands\nai --shell \"Create a backup of my Documents folder\"\n\n# Generate code snippets (Python by default)\nai --code \"Write a function to sort a list\"\n\n# Use a specific LLM model\nai --model gpt-4-turbo \"Explain quantum computing\"\n\n# Adjust temperature for more creative responses\nai --temperature 0.8 \"Write a poem about AI\"\n\n# Create a custom role\nai --create-role \"SQL Expert\"\n\n# Use a custom role\nai --role \"SQL Expert\" \"Optimize this query\"\n\n# Show verbose output for debugging\nai --verbose \"Test query\"\n</code></pre>"},{"location":"usage/commands/#interactive-mode-commands","title":"Interactive Mode Commands","text":"<p>When in interactive chat or shell mode, you can use these special commands:</p> Command Description <code>/help</code> or <code>/?</code> Show help message <code>/clear</code> Clear conversation history <code>/his</code> Show command history <code>/list</code> List saved chats <code>/save &lt;title&gt;</code> Save current chat with title <code>/load &lt;index&gt;</code> Load a saved chat <code>/del &lt;index&gt;</code> Delete a saved chat <code>/exit</code> Exit the application <code>/mode chat\\|exec</code> Switch between chat and execute modes"},{"location":"usage/commands/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"Shortcut Description <code>Tab</code> Toggle between Chat/Execute modes <code>Ctrl+C</code> or <code>Ctrl+D</code> Exit <code>Ctrl+R</code> Search history <code>\u2191/\u2193</code> Navigate through history"},{"location":"usage/commands/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about configuration options</li> <li>Explore different interaction modes</li> <li>Discover CLI shortcuts and hotkeys</li> </ul>"},{"location":"usage/configuration/","title":"Configuration Guide","text":"<p>YAICLI uses a layered configuration system to store your preferences and API keys. This guide explains all available configuration options and how to customize them.</p>"},{"location":"usage/configuration/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<p>YAICLI follows this priority order for configuration:</p> <ol> <li>Command-line arguments (highest priority)</li> <li>Environment variables</li> <li>Configuration file</li> <li>Default values (lowest priority)</li> </ol>"},{"location":"usage/configuration/#configuration-file","title":"Configuration File","text":""},{"location":"usage/configuration/#location","title":"Location","text":"<p>The default configuration file is located at:</p> <ul> <li>Linux/macOS: <code>~/.config/yaicli/config.ini</code></li> <li>Windows: <code>C:\\Users\\&lt;username&gt;\\.config\\yaicli\\config.ini</code></li> </ul>"},{"location":"usage/configuration/#first-time-setup","title":"First-time Setup","text":"<ol> <li>Run <code>ai</code> once to generate the default configuration file</li> <li>Edit the file to add your API key and customize settings</li> </ol>"},{"location":"usage/configuration/#viewing-default-configuration","title":"Viewing Default Configuration","text":"<p>To view the default configuration template:</p> <pre><code>ai --template\n</code></pre>"},{"location":"usage/configuration/#configuration-file-structure","title":"Configuration File Structure","text":"<p>The configuration file uses the INI format with the following sections:</p> <pre><code>[core]\nPROVIDER=openai\nBASE_URL=\nAPI_KEY=\nMODEL=gpt-4o\n\nDEFAULT_ROLE=DEFAULT\n# auto detect shell and os (or specify manually, e.g., bash, zsh, powershell.exe)\nSHELL_NAME=auto\nOS_NAME=auto\n\n# true: streaming response, false: non-streaming\nSTREAM=true\n\n# LLM parameters\nTEMPERATURE=0.3\nTOP_P=1.0\nMAX_TOKENS=1024\nTIMEOUT=60\nREASONING_EFFORT=\n\n# Interactive mode parameters\nINTERACTIVE_ROUND=25\n\n# UI/UX\nCODE_THEME=monokai\n# Max entries kept in history file\nMAX_HISTORY=500\nAUTO_SUGGEST=true\n# Print reasoning content or not\nSHOW_REASONING=true\n# Text alignment (default, left, center, right, full)\nJUSTIFY=default\n\n# Chat history settings\nCHAT_HISTORY_DIR=&lt;tmpdir&gt;/yaicli/chats\nMAX_SAVED_CHATS=20\n\n# Role settings\n# Set to false to disable warnings about modified built-in roles\nROLE_MODIFY_WARNING=true\n\n# Function settings\n# Set to false to disable sending functions in API requests\nENABLE_FUNCTIONS=true\n# Set to false to disable showing function output in the response\nSHOW_FUNCTION_OUTPUT=true\n\n# MCP settings\nENABLE_MCP=false\nSHOW_MCP_OUTPUT=false\n</code></pre>"},{"location":"usage/configuration/#configuration-options-reference","title":"Configuration Options Reference","text":"Option Description Default Environment Variable <code>PROVIDER</code> LLM provider (openai, claude, cohere, etc.) <code>openai</code> <code>YAI_PROVIDER</code> <code>BASE_URL</code> API endpoint URL - <code>YAI_BASE_URL</code> <code>API_KEY</code> Your API key - <code>YAI_API_KEY</code> <code>MODEL</code> LLM model to use <code>gpt-4o</code> <code>YAI_MODEL</code> <code>DEFAULT_ROLE</code> Default role <code>DEFAULT</code> <code>YAI_DEFAULT_ROLE</code> <code>SHELL_NAME</code> Shell type <code>auto</code> <code>YAI_SHELL_NAME</code> <code>OS_NAME</code> Operating system <code>auto</code> <code>YAI_OS_NAME</code> <code>STREAM</code> Enable streaming <code>true</code> <code>YAI_STREAM</code> <code>TIMEOUT</code> API timeout (seconds) <code>60</code> <code>YAI_TIMEOUT</code> <code>EXTRA_HEADERS</code> Extra headers - <code>YAI_EXTRA_HEADERS</code> <code>EXTRA_BODY</code> Extra body - <code>YAI_EXTRA_BODY</code> <code>REASONING_EFFORT</code> Reasoning effort - <code>YAI_REASONING_EFFORT</code> <code>INTERACTIVE_ROUND</code> Interactive mode rounds <code>25</code> <code>YAI_INTERACTIVE_ROUND</code> <code>CODE_THEME</code> Syntax highlighting theme <code>monokai</code> <code>YAI_CODE_THEME</code> <code>TEMPERATURE</code> Response randomness <code>0.3</code> <code>YAI_TEMPERATURE</code> <code>TOP_P</code> Top-p sampling <code>1.0</code> <code>YAI_TOP_P</code> <code>MAX_TOKENS</code> Max response tokens <code>1024</code> <code>YAI_MAX_TOKENS</code> <code>MAX_HISTORY</code> Max history entries <code>500</code> <code>YAI_MAX_HISTORY</code> <code>AUTO_SUGGEST</code> Enable history suggestions <code>true</code> <code>YAI_AUTO_SUGGEST</code> <code>SHOW_REASONING</code> Enable reasoning display <code>true</code> <code>YAI_SHOW_REASONING</code> <code>JUSTIFY</code> Text alignment <code>default</code> <code>YAI_JUSTIFY</code> <code>CHAT_HISTORY_DIR</code> Chat history directory <code>&lt;tempdir&gt;/yaicli/chats</code> <code>YAI_CHAT_HISTORY_DIR</code> <code>MAX_SAVED_CHATS</code> Max saved chats <code>20</code> <code>YAI_MAX_SAVED_CHATS</code> <code>ROLE_MODIFY_WARNING</code> Warn when modifying built-in roles <code>true</code> <code>YAI_ROLE_MODIFY_WARNING</code> <code>ENABLE_FUNCTIONS</code> Enable function calling <code>true</code> <code>YAI_ENABLE_FUNCTIONS</code> <code>SHOW_FUNCTION_OUTPUT</code> Show function output <code>true</code> <code>YAI_SHOW_FUNCTION_OUTPUT</code> <code>ENABLE_MCP</code> Enable MCP tools <code>false</code> <code>YAI_ENABLE_MCP</code> <code>SHOW_MCP_OUTPUT</code> Show MCP output <code>true</code> <code>YAI_SHOW_MCP_OUTPUT</code>"},{"location":"usage/configuration/#syntax-highlighting-themes","title":"Syntax Highlighting Themes","text":"<p>YAICLI supports all Pygments syntax highlighting themes. Set your preferred theme with:</p> <pre><code>CODE_THEME = monokai\n</code></pre> <p>Browse available themes at: https://pygments.org/styles/</p>"},{"location":"usage/configuration/#extra-headers-and-body","title":"Extra Headers and Body","text":"<p>You can add extra headers and body parameters to the API request:</p> <pre><code>EXTRA_HEADERS={\"X-Extra-Header\": \"value\"}\nEXTRA_BODY={\"extra_key\": \"extra_value\"}\n</code></pre> <p>Example: Disable Qwen3's thinking behavior:</p> <pre><code>EXTRA_BODY={\"enable_thinking\": false}\n</code></pre> <p>Or limit thinking tokens:</p> <pre><code>EXTRA_BODY={\"thinking_budget\": 4096}\n</code></pre>"},{"location":"usage/configuration/#environment-variables","title":"Environment Variables","text":"<p>All configuration options can be set using environment variables with the <code>YAI_</code> prefix:</p> <pre><code># Set API key via environment variable\nexport YAI_API_KEY=\"your-api-key\"\n\n# Use a different model\nexport YAI_MODEL=\"gpt-3.5-turbo\"\n\n# Disable streaming\nexport YAI_STREAM=\"false\"\n</code></pre> <p>Environment variables take precedence over the configuration file settings.</p>"},{"location":"usage/configuration/#provider-specific-configuration","title":"Provider-Specific Configuration","text":"<p>See the Providers section for detailed configuration options for each supported LLM provider.</p>"},{"location":"usage/interactive/","title":"Interactive Mode","text":"<p>YAICLI provides an interactive mode where you can have ongoing conversations with AI models.</p> <p>Run <code>ai --chat &lt;title&gt;</code> to start interactive session.</p> <pre><code>ai --chat \"What's the meaning of life\"\n\n \u2588\u2588    \u2588\u2588  \u2588\u2588\u2588\u2588\u2588  \u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588      \u2588\u2588\n  \u2588\u2588  \u2588\u2588  \u2588\u2588   \u2588\u2588 \u2588\u2588 \u2588\u2588      \u2588\u2588      \u2588\u2588\n   \u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588      \u2588\u2588      \u2588\u2588\n    \u2588\u2588    \u2588\u2588   \u2588\u2588 \u2588\u2588 \u2588\u2588      \u2588\u2588      \u2588\u2588\n    \u2588\u2588    \u2588\u2588   \u2588\u2588 \u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\n\nWelcome to YAICLI!\nCurrent: Persistent Session: What's the meaning of life\nPress TAB to switch mode\n/help|?            : Show help message\n/clear             : Clear chat history\n/his               : Show chat history\n/list              : List saved chats\n/save &lt;title&gt;      : Save current chat\n/load &lt;index&gt;      : Load a saved chat\n/del &lt;index&gt;       : Delete a saved chat\n/exit|Ctrl+D|Ctrl+C: Exit\n/mode chat|exec    : Switch mode (Case insensitive)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n \ud83d\udcac &gt;\n</code></pre> <p>Or use <code>ai --chat</code> to start a temporary session.</p> <pre><code>ai --chat\n\n \u2588\u2588    \u2588\u2588  \u2588\u2588\u2588\u2588\u2588  \u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588      \u2588\u2588\n  \u2588\u2588  \u2588\u2588  \u2588\u2588   \u2588\u2588 \u2588\u2588 \u2588\u2588      \u2588\u2588      \u2588\u2588\n   \u2588\u2588\u2588\u2588   \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588      \u2588\u2588      \u2588\u2588\n    \u2588\u2588    \u2588\u2588   \u2588\u2588 \u2588\u2588 \u2588\u2588      \u2588\u2588      \u2588\u2588\n    \u2588\u2588    \u2588\u2588   \u2588\u2588 \u2588\u2588  \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\n\nWelcome to YAICLI!\nCurrent: Temporary Session (use /save to make persistent)\nPress TAB to switch mode\n/help|?            : Show help message\n/clear             : Clear chat history\n/his               : Show chat history\n/list              : List saved chats\n/save &lt;title&gt;      : Save current chat\n/load &lt;index&gt;      : Load a saved chat\n/del &lt;index&gt;       : Delete a saved chat\n/exit|Ctrl+D|Ctrl+C: Exit\n/mode chat|exec    : Switch mode (Case insensitive)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n \ud83d\udcac &gt;\n</code></pre>"},{"location":"usage/interactive/#commands","title":"Commands","text":"<ul> <li><code>/help|?</code> - Show help message</li> <li><code>/clear</code> - Clear conversation history</li> <li><code>/his</code> - Show command history</li> <li><code>/list</code> - List saved chats</li> <li><code>/save &lt;title&gt;</code> - Save current chat with title</li> <li><code>/load &lt;index&gt;</code> - Load a saved chat</li> <li><code>/del &lt;index&gt;</code> - Delete a saved chat</li> <li><code>/exit</code> - Exit the application</li> <li><code>/mode chat|exec</code> - Switch modes</li> </ul>"},{"location":"usage/interactive/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"<ul> <li><code>Tab</code> - Toggle between Chat/Execute modes</li> <li><code>Ctrl+C</code> or <code>Ctrl+D</code> - Exit</li> <li><code>Ctrl+R</code> - Search history</li> <li><code>\u2191/\u2193</code> - Navigate through history</li> </ul>"},{"location":"usage/interactive/#chat-mode","title":"Chat Mode (\ud83d\udcac)","text":"<p>In Chat Mode, you can have natural conversations with the AI while maintaining context: - Full conversation history is maintained - Markdown and code formatting is supported - Reasoning display for complex queries is available</p> <pre><code>$ ai --chat\n...\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n \ud83d\udcac &gt; Tell me about the solar system\n\nA:\nSolar System Overview\n\n \u2022 Central Star: The Sun (99% of system mass, nuclear fusion).\n \u2022 Planets: 8 total.\n    \u2022 Terrestrial (rocky): Mercury, Venus, Earth, Mars.\n    \u2022 Gas Giants: Jupiter, Saturn.\n    \u2022 Ice Giants: Uranus, Neptune.\n \u2022 Moons: Over 200 (e.g., Earth: 1, Jupiter: 95).\n \u2022 Smaller Bodies:\n    \u2022 Asteroids (between Mars/Venus), comets ( icy, distant), * dwarf planets* (Pluto, Ceres).\n \u2022 Oort Cloud: spherical shell of icy objects ~1\u2013100,000\u5929\u6587\u55ae\u4f4d (AU) from Sun).\n \u2022 Heliosphere: Solar wind boundary protecting Earth from cosmic radiation.\n\nKey Fact: Earth is the only confirmed habitable planet.\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre>"},{"location":"usage/interactive/#execute-mode","title":"Execute Mode (\ud83d\ude80)","text":"<p>Execute Mode allows you to generate shell commands from natural language descriptions: - Generate shell commands from descriptions - Review commands before execution - Edit commands before running - Safe execution with confirmation</p> <pre><code>\ud83d\ude80 &gt; Find all PDF files in my Downloads folder\nAssistant:\nfind ~/Downloads -type f -name \"*.pdf\"\n\u256d\u2500 Suggest Command \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 find ~/Downloads -type f -name \"*.pdf\" \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nExecute command? [e]dit, [y]es, [n]o (y):\nExecuting...\n/Users/username/Downloads/document1.pdf\n/Users/username/Downloads/report.pdf\n...\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre>"},{"location":"usage/interactive/#saved-chat","title":"Saved Chat","text":"<p>Your chats will save in <code>CHAT_HISTORY_DIR</code> key in <code>~/.config/yaicli/config.ini</code>.</p> <p>You can use <code>ai --template</code> to see default <code>CHAT_HISTORY_DIR</code> value, it's a temp fold created by Python, will save in <code>config.ini</code> when you first run <code>ai</code>.</p>"},{"location":"usage/modes/","title":"Mode Switching (Chat / Execute)","text":"<p>YAICLI offers multiple interaction modes to suit different needs. This page explains each mode and how to switch between them.</p>"},{"location":"usage/modes/#available-modes","title":"Available Modes","text":"<p>YAICLI has three primary interaction modes:</p>"},{"location":"usage/modes/#1-chat-mode","title":"1. Chat Mode (\ud83d\udcac)","text":"<p>Chat mode provides a natural conversation experience with the LLM, maintaining context and formatting responses with markdown.</p> <p>Features: - Full conversation history and context tracking - Markdown and code formatting - Reasoning display for complex queries - Persistent chat storage and management</p> <p>Best for: - Extended conversations - Complex questions requiring context - Learning and exploration</p>"},{"location":"usage/modes/#2-execute-mode","title":"2. Execute Mode (\ud83d\ude80)","text":"<p>Execute mode specializes in generating and safely running shell commands based on natural language descriptions.</p> <p>Features: - OS-specific command generation - Command review and editing before execution - Command output display - Safe execution with confirmation</p> <p>Best for: - System administration tasks - File operations - Command discovery - Automating repetitive tasks</p>"},{"location":"usage/modes/#3-quick-query-mode-default","title":"3. Quick Query Mode (Default)","text":"<p>The default non-interactive mode gives you quick answers without entering a persistent session.</p> <p>Features: - Direct answers to questions - Support for piped input - Fast responses with minimal overhead</p> <p>Best for: - One-off questions - Quick information lookups - Script integration</p>"},{"location":"usage/modes/#switching-between-modes","title":"Switching Between Modes","text":""},{"location":"usage/modes/#command-line-switches","title":"Command Line Switches","text":"<p>Use these flags when starting YAICLI:</p> <pre><code># Start in chat mode\nai --chat \"Optional chat title\"\n\n# Start in execute mode\nai --shell \"Create a backup script\"\n\n# Quick query (default mode)\nai \"What is the capital of France?\"\n</code></pre>"},{"location":"usage/modes/#interactive-mode-switching","title":"Interactive Mode Switching","text":"<p>When in interactive mode (chat or execute), you can switch modes in several ways:</p>"},{"location":"usage/modes/#using-tab-key","title":"Using Tab Key","text":"<p>Press the <code>Tab</code> key to toggle between Chat and Execute modes.</p>"},{"location":"usage/modes/#using-commands","title":"Using Commands","text":"<p>Type <code>/mode</code> followed by the mode name:</p> <pre><code>/mode chat   # Switch to chat mode\n/mode exec   # Switch to execute mode\n</code></pre>"},{"location":"usage/modes/#visual-indicators","title":"Visual Indicators","text":"<p>The prompt changes to indicate your current mode: - Chat mode: <code>\ud83d\udcac &gt;</code> - Execute mode: <code>\ud83d\ude80 &gt;</code></p>"},{"location":"usage/modes/#mode-behavior","title":"Mode Behavior","text":""},{"location":"usage/modes/#chat-mode-behavior","title":"Chat Mode Behavior","text":"<p>In chat mode: - The AI responds conversationally with rich formatting - Context is maintained between interactions - Input is treated as a conversation prompt</p> <p>Example: <pre><code>\ud83d\udcac &gt; Tell me about quantum computing\nAssistant:\nQuantum computing uses quantum bits (qubits) that can exist in multiple \nstates simultaneously due to superposition...\n</code></pre></p>"},{"location":"usage/modes/#execute-mode-behavior","title":"Execute Mode Behavior","text":"<p>In execute mode: - The AI interprets input as a request for a command - Generates appropriate commands for your OS and shell - Displays the command for review before execution</p> <p>Example: <pre><code>\ud83d\ude80 &gt; Find all PDF files in my Downloads folder\nAssistant:\nfind ~/Downloads -name \"*.pdf\"\n\u256d\u2500 Suggest Command \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 find ~/Downloads -name \"*.pdf\"        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nExecute command? [e]dit, [y]es, [n]o (n): \n</code></pre></p>"},{"location":"usage/modes/#chat-persistence","title":"Chat Persistence","text":"<p>When starting a chat session with a title, the conversation is automatically saved:</p> <pre><code>ai --chat \"Python tips\"\n</code></pre> <p>A chat started without a title is treated as temporary but can be saved later:</p> <pre><code>\ud83d\udcac &gt; /save \"Python tips\"\nChat saved as: Python tips\n</code></pre> <p>You can list, load, and manage saved chats:</p> <pre><code>\ud83d\udcac &gt; /list\nSaved chats:\n1. Python tips (2023-07-01)\n2. Work project (2023-06-28)\n\n\ud83d\udcac &gt; /load 1\nLoading chat: Python tips\n</code></pre>"},{"location":"usage/modes/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about CLI shortcuts and hotkeys</li> <li>Explore configuration options</li> <li>Check out all available commands</li> </ul>"}]}